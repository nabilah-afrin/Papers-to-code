{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MAE are scalable ViT"]},{"cell_type":"markdown","metadata":{},"source":["The author experiments on how suprevised learning can be achieved through the ViT, as \"masked auto encoder\". The task of MAE is to recosntuct the missing patches of an image."]},{"cell_type":"markdown","metadata":{},"source":["# 1. import dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-09T00:58:25.496620Z","iopub.status.busy":"2024-05-09T00:58:25.496254Z","iopub.status.idle":"2024-05-09T00:58:27.076443Z","shell.execute_reply":"2024-05-09T00:58:27.075223Z","shell.execute_reply.started":"2024-05-09T00:58:25.496587Z"},"trusted":true},"outputs":[],"source":["!git clone https://github.com/pengzhiliang/MAE-pytorch.git"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T00:58:27.088806Z","iopub.status.busy":"2024-05-09T00:58:27.088360Z","iopub.status.idle":"2024-05-09T00:58:28.098688Z","shell.execute_reply":"2024-05-09T00:58:28.097353Z","shell.execute_reply.started":"2024-05-09T00:58:27.088780Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch==1.7.1 # the later version is also OK\n","torchvision==0.8.2\n","timm==0.4.12\n","Pillow\n","blobfile\n","mypy\n","numpy\n","pytest\n","requests\n","einops\n","tensorboardX\n","# deepspeed==0.4.0\n","scipy\n"]}],"source":["!cat requirements.txt"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T00:58:28.102375Z","iopub.status.busy":"2024-05-09T00:58:28.101924Z","iopub.status.idle":"2024-05-09T00:58:29.105510Z","shell.execute_reply":"2024-05-09T00:58:29.104169Z","shell.execute_reply.started":"2024-05-09T00:58:28.102330Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.10.13\n"]}],"source":["!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create New Conda Environment and Use Conda Channel \n","!conda create -n py_3_8 -c cctbx202208 python=3.8 -y\n","!source /opt/conda/bin/activate py_3_8 && conda install -c cctbx202208 python -y"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T00:59:53.332356Z","iopub.status.busy":"2024-05-09T00:59:53.331896Z","iopub.status.idle":"2024-05-09T00:59:53.342150Z","shell.execute_reply":"2024-05-09T00:59:53.341128Z","shell.execute_reply.started":"2024-05-09T00:59:53.332307Z"},"trusted":true},"outputs":[],"source":["# !/opt/conda/envs/py_3_8/bin/python3 --version\n","# !echo 'print(\"Hello, World!\")' > test.py\n","# !/opt/conda/envs/py_3_8/bin/python3 test.py"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T00:59:53.344401Z","iopub.status.busy":"2024-05-09T00:59:53.343649Z","iopub.status.idle":"2024-05-09T00:59:53.389974Z","shell.execute_reply":"2024-05-09T00:59:53.388553Z","shell.execute_reply.started":"2024-05-09T00:59:53.344360Z"},"trusted":true},"outputs":[],"source":["# !conda activate py_3_8"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T00:59:53.392104Z","iopub.status.busy":"2024-05-09T00:59:53.391404Z","iopub.status.idle":"2024-05-09T00:59:53.400868Z","shell.execute_reply":"2024-05-09T00:59:53.399827Z","shell.execute_reply.started":"2024-05-09T00:59:53.392073Z"},"trusted":true},"outputs":[],"source":["# !sudo rm /opt/conda/bin/python3\n","# !sudo ln -sf /opt/conda/envs/py_3_8/bin/python3 /opt/conda/bin/python3"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T00:59:53.402946Z","iopub.status.busy":"2024-05-09T00:59:53.402467Z","iopub.status.idle":"2024-05-09T00:59:53.411513Z","shell.execute_reply":"2024-05-09T00:59:53.410550Z","shell.execute_reply.started":"2024-05-09T00:59:53.402911Z"},"trusted":true},"outputs":[],"source":["# !sudo rm /opt/conda/bin/python3.10\n","# !sudo ln -sf /opt/conda/envs/py_3_8/bin/python3 /opt/conda/bin/python3.10"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T00:59:53.415799Z","iopub.status.busy":"2024-05-09T00:59:53.415473Z","iopub.status.idle":"2024-05-09T00:59:53.420930Z","shell.execute_reply":"2024-05-09T00:59:53.420035Z","shell.execute_reply.started":"2024-05-09T00:59:53.415750Z"},"trusted":true},"outputs":[],"source":["# !sudo rm /opt/conda/bin/python\n","# !sudo ln -s /opt/conda/envs/py_3_8/bin/python3 /opt/conda/bin/python"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T00:59:53.422582Z","iopub.status.busy":"2024-05-09T00:59:53.422101Z","iopub.status.idle":"2024-05-09T00:59:53.432666Z","shell.execute_reply":"2024-05-09T00:59:53.431574Z","shell.execute_reply.started":"2024-05-09T00:59:53.422555Z"},"trusted":true},"outputs":[],"source":["# !python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install -r requirements.txt --no-warn-script-location"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T00:59:56.055306Z","iopub.status.busy":"2024-05-09T00:59:56.054968Z","iopub.status.idle":"2024-05-09T01:00:08.813914Z","shell.execute_reply":"2024-05-09T01:00:08.812931Z","shell.execute_reply.started":"2024-05-09T00:59:56.055272Z"},"trusted":true},"outputs":[],"source":["# !pip install torchsummary"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:08.816195Z","iopub.status.busy":"2024-05-09T01:00:08.815882Z","iopub.status.idle":"2024-05-09T01:00:14.795452Z","shell.execute_reply":"2024-05-09T01:00:14.794330Z","shell.execute_reply.started":"2024-05-09T01:00:08.816163Z"},"trusted":true},"outputs":[],"source":["import math\n","import torch\n","import torch.nn as nn #import the neural network modules\n","import torch.nn.functional as F #for activation functions and other operations\n","# from timm.models.registry import register_model\n","# from timm.models.layers import trunc_normal_ as __call_trunc_normal_ # to initializing the weights of linear layers and convolution\n","import numpy as np\n","from functools import partial\n","from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n","from timm.models.layers import trunc_normal_ as __call_trunc_normal_"]},{"cell_type":"markdown","metadata":{},"source":["# Model Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## Embedding"]},{"cell_type":"markdown","metadata":{},"source":["base model is just the adaptation of vit. let's look at how images are being encoded in ViT\n","\n","\n","<img src = \"https://d2l.ai/_images/vit.svg\">"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 patch embedding"]},{"cell_type":"markdown","metadata":{},"source":["So, at first we need to patchify the image"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.797161Z","iopub.status.busy":"2024-05-09T01:00:14.796736Z","iopub.status.idle":"2024-05-09T01:00:14.807121Z","shell.execute_reply":"2024-05-09T01:00:14.805937Z","shell.execute_reply.started":"2024-05-09T01:00:14.797126Z"},"trusted":true},"outputs":[],"source":["class PatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n","        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, x, **kwargs):\n","        B, C, H, W = x.shape\n","        \n","        assert H == self.img_size[0] and W == self.img_size[1], \\\n","            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n","        x = self.proj(x).flatten(2).transpose(1, 2)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2. Positional Embedding"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.809190Z","iopub.status.busy":"2024-05-09T01:00:14.808879Z","iopub.status.idle":"2024-05-09T01:00:14.823303Z","shell.execute_reply":"2024-05-09T01:00:14.822222Z","shell.execute_reply.started":"2024-05-09T01:00:14.809165Z"},"trusted":true},"outputs":[],"source":["def get_sinusoid_encoding_table(n_position, d_hid): \n","    ''' Sinusoid position encoding table ''' \n","\n","    def get_position_angle_vec(position): \n","        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] \n","\n","    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)]) \n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i \n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1 \n","\n","    return torch.FloatTensor(sinusoid_table).unsqueeze(0) "]},{"cell_type":"markdown","metadata":{},"source":["<img src = \"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*2detiL9GQmEYEyUTaYJoLA.png\">"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.826005Z","iopub.status.busy":"2024-05-09T01:00:14.824947Z","iopub.status.idle":"2024-05-09T01:00:14.835990Z","shell.execute_reply":"2024-05-09T01:00:14.834720Z","shell.execute_reply.started":"2024-05-09T01:00:14.825958Z"},"trusted":true},"outputs":[],"source":["def _cfg(url='', **kwargs):\n","    return {\n","        'url': url,\n","        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n","        'crop_pct': .9, 'interpolation': 'bicubic',\n","        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n","        **kwargs\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["## Transformer Block"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.838367Z","iopub.status.busy":"2024-05-09T01:00:14.837916Z","iopub.status.idle":"2024-05-09T01:00:14.846851Z","shell.execute_reply":"2024-05-09T01:00:14.845960Z","shell.execute_reply.started":"2024-05-09T01:00:14.838324Z"},"trusted":true},"outputs":[],"source":["class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        # x = self.drop(x)\n","        # commit this for the orignal BERT implement \n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["### Multihead Attention"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.848646Z","iopub.status.busy":"2024-05-09T01:00:14.848292Z","iopub.status.idle":"2024-05-09T01:00:14.865103Z","shell.execute_reply":"2024-05-09T01:00:14.863556Z","shell.execute_reply.started":"2024-05-09T01:00:14.848619Z"},"trusted":true},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(\n","            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n","            proj_drop=0., attn_head_dim=None):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        if attn_head_dim is not None:\n","            head_dim = attn_head_dim\n","        all_head_dim = head_dim * self.num_heads\n","        self.scale = qk_scale or head_dim ** -0.5 #attention score\n","        \n","    # Linear transformation for queries, keys, and values\n","\n","        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n","        if qkv_bias:\n","            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n","            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n","        else:\n","            self.q_bias = None\n","            self.v_bias = None\n","            \n","# extract the patches from the image and the normalized attention score\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(all_head_dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv_bias = None\n","        if self.q_bias is not None:\n","            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n","\n","        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n","        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]   \n","\n","        q = q * self.scale\n","        \n","        #             linear tranformation\n","\n","        attn = (q @ k.transpose(-2, -1))\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","        \n","# add the attention score\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.867648Z","iopub.status.busy":"2024-05-09T01:00:14.866978Z","iopub.status.idle":"2024-05-09T01:00:14.883710Z","shell.execute_reply":"2024-05-09T01:00:14.882470Z","shell.execute_reply.started":"2024-05-09T01:00:14.867613Z"},"trusted":true},"outputs":[],"source":["class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n","                 attn_head_dim=None):\n","        super().__init__()\n","        \n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        if init_values > 0:\n","            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","        else:\n","            self.gamma_1, self.gamma_2 = None, None\n","\n","    def forward(self, x):\n","        if self.gamma_1 is None:\n","            x = x + self.drop_path(self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        else:\n","            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["# Encoder"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.886245Z","iopub.status.busy":"2024-05-09T01:00:14.885387Z","iopub.status.idle":"2024-05-09T01:00:14.904001Z","shell.execute_reply":"2024-05-09T01:00:14.903056Z","shell.execute_reply.started":"2024-05-09T01:00:14.886215Z"},"trusted":true},"outputs":[],"source":["class PretrainVisionTransformerEncoder(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n","                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n","                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None,\n","                 use_learnable_pos_emb=False):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","\n","        \n","#         patch embedding layer\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n","        num_patches = self.patch_embed.num_patches\n","\n","# positional encoding       \n","        if use_learnable_pos_emb:\n","            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n","        else:\n","            # sine-cosine positional embeddings \n","            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n","\n","            \n","#             Transformer blocks\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        \n","#         classification\n","        self.norm =  norm_layer(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        if use_learnable_pos_emb:\n","            trunc_normal_(self.pos_embed, std=.02)\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x, mask):\n","        x = self.patch_embed(x)\n","        \n","        # cls_tokens = self.cls_token.expand(batch_size, -1, -1) \n","        # x = torch.cat((cls_tokens, x), dim=1)\n","        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n","\n","        B, _, C = x.shape\n","        x_vis = x[~mask].reshape(B, -1, C) # ~mask means visible\n","\n","        for blk in self.blocks:\n","            x_vis = blk(x_vis)\n","\n","        x_vis = self.norm(x_vis)\n","        return x_vis\n","\n","    def forward(self, x, mask):\n","        x = self.forward_features(x, mask)\n","        x = self.head(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["# Decoder"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.906099Z","iopub.status.busy":"2024-05-09T01:00:14.905294Z","iopub.status.idle":"2024-05-09T01:00:14.923878Z","shell.execute_reply":"2024-05-09T01:00:14.922516Z","shell.execute_reply.started":"2024-05-09T01:00:14.906067Z"},"trusted":true},"outputs":[],"source":["class PretrainVisionTransformerDecoder(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, patch_size=16, num_classes=768, embed_dim=768, depth=12,\n","                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n","                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, num_patches=196,\n","                 ):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        assert num_classes == 3 * patch_size ** 2\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.patch_size = patch_size\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm =  norm_layer(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward(self, x, return_token_num):\n","        for blk in self.blocks:\n","            x = blk(x)\n","\n","        if return_token_num > 0:\n","            x = self.head(self.norm(x[:, -return_token_num:])) # only return the mask tokens predict pixels\n","        else:\n","            x = self.head(self.norm(x)) # [B, N, 3*16^2]\n","\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["# MAE"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.929708Z","iopub.status.busy":"2024-05-09T01:00:14.928716Z","iopub.status.idle":"2024-05-09T01:00:14.948983Z","shell.execute_reply":"2024-05-09T01:00:14.947738Z","shell.execute_reply.started":"2024-05-09T01:00:14.929665Z"},"trusted":true},"outputs":[],"source":["class PretrainVisionTransformer(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self,\n","                 img_size=224, \n","                 patch_size=16, \n","                 encoder_in_chans=3, \n","                 encoder_num_classes=0, \n","                 encoder_embed_dim=768, \n","                 encoder_depth=12,\n","                 encoder_num_heads=12, \n","                 decoder_num_classes=768, \n","                 decoder_embed_dim=512, \n","                 decoder_depth=8,\n","                 decoder_num_heads=8, \n","                 mlp_ratio=4., \n","                 qkv_bias=False, \n","                 qk_scale=None, \n","                 drop_rate=0., \n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0., \n","                 norm_layer=nn.LayerNorm, \n","                 init_values=0.,\n","                 use_learnable_pos_emb=False,\n","                 num_classes=0, # avoid the error from create_fn in timm\n","                 in_chans=0, # avoid the error from create_fn in timm\n","                 ):\n","        super().__init__()\n","        self.encoder = PretrainVisionTransformerEncoder(\n","            img_size=img_size, \n","            patch_size=patch_size, \n","            in_chans=encoder_in_chans, \n","            num_classes=encoder_num_classes, \n","            embed_dim=encoder_embed_dim, \n","            depth=encoder_depth,\n","            num_heads=encoder_num_heads, \n","            mlp_ratio=mlp_ratio, \n","            qkv_bias=qkv_bias, \n","            qk_scale=qk_scale, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=drop_path_rate, \n","            norm_layer=norm_layer, \n","            init_values=init_values,\n","            use_learnable_pos_emb=use_learnable_pos_emb)\n","\n","        self.decoder = PretrainVisionTransformerDecoder(\n","            patch_size=patch_size, \n","            num_patches=self.encoder.patch_embed.num_patches,\n","            num_classes=decoder_num_classes, \n","            embed_dim=decoder_embed_dim, \n","            depth=decoder_depth,\n","            num_heads=decoder_num_heads, \n","            mlp_ratio=mlp_ratio, \n","            qkv_bias=qkv_bias, \n","            qk_scale=qk_scale, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=drop_path_rate, \n","            norm_layer=norm_layer, \n","            init_values=init_values)\n","\n","        self.encoder_to_decoder = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=False)\n","\n","        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n","\n","        self.pos_embed = get_sinusoid_encoding_table(self.encoder.patch_embed.num_patches, decoder_embed_dim)\n","\n","        trunc_normal_(self.mask_token, std=.02)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token', 'mask_token'}\n","\n","    def forward(self, x, mask):\n","        \n","        x_vis = self.encoder(x, mask) # [B, N_vis, C_e]\n","        x_vis = self.encoder_to_decoder(x_vis) # [B, N_vis, C_d]\n","\n","        B, N, C = x_vis.shape\n","        \n","        # we don't unshuffle the correct visible token order, \n","        # but shuffle the pos embedding accorddingly.\n","        expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","        pos_emd_vis = expand_pos_embed[~mask].reshape(B, -1, C)\n","        pos_emd_mask = expand_pos_embed[mask].reshape(B, -1, C)\n","        x_full = torch.cat([x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1)\n","        # notice: if N_mask==0, the shape of x is [B, N_mask, 3 * 16 * 16]\n","        x = self.decoder(x_full, pos_emd_mask.shape[1]) # [B, N_mask, 3 * 16 * 16]\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## Model Variants"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.951252Z","iopub.status.busy":"2024-05-09T01:00:14.950587Z","iopub.status.idle":"2024-05-09T01:00:14.966345Z","shell.execute_reply":"2024-05-09T01:00:14.965392Z","shell.execute_reply.started":"2024-05-09T01:00:14.951213Z"},"trusted":true},"outputs":[],"source":["def print_parameters(model):\n","    for name, param in model.named_parameters():\n","        print(name, param.data.shape)\n","    print(\"=============================================================================================\")\n","    total_params = sum(p.numel() for p in model.parameters())\n","    print(f\"Total number of parameters: {total_params}\")"]},{"cell_type":"markdown","metadata":{},"source":["### mae small patch"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.982508Z","iopub.status.busy":"2024-05-09T01:00:14.981906Z","iopub.status.idle":"2024-05-09T01:00:14.991141Z","shell.execute_reply":"2024-05-09T01:00:14.990145Z","shell.execute_reply.started":"2024-05-09T01:00:14.982477Z"},"trusted":true},"outputs":[],"source":["def pretrain_mae_small_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16,\n","        encoder_embed_dim=384,\n","        encoder_depth=12,\n","        encoder_num_heads=6,\n","        encoder_num_classes=0,\n","        decoder_num_classes=768,\n","        decoder_embed_dim=192,\n","        decoder_depth=4,\n","        decoder_num_heads=3,\n","        mlp_ratio=4,\n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:14.993170Z","iopub.status.busy":"2024-05-09T01:00:14.992269Z","iopub.status.idle":"2024-05-09T01:00:15.799967Z","shell.execute_reply":"2024-05-09T01:00:15.798787Z","shell.execute_reply.started":"2024-05-09T01:00:14.993119Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mask_token torch.Size([1, 1, 192])\n","encoder.patch_embed.proj.weight torch.Size([384, 3, 16, 16])\n","encoder.patch_embed.proj.bias torch.Size([384])\n","encoder.blocks.0.norm1.weight torch.Size([384])\n","encoder.blocks.0.norm1.bias torch.Size([384])\n","encoder.blocks.0.attn.q_bias torch.Size([384])\n","encoder.blocks.0.attn.v_bias torch.Size([384])\n","encoder.blocks.0.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.0.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.0.attn.proj.bias torch.Size([384])\n","encoder.blocks.0.norm2.weight torch.Size([384])\n","encoder.blocks.0.norm2.bias torch.Size([384])\n","encoder.blocks.0.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.0.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.0.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.0.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.1.norm1.weight torch.Size([384])\n","encoder.blocks.1.norm1.bias torch.Size([384])\n","encoder.blocks.1.attn.q_bias torch.Size([384])\n","encoder.blocks.1.attn.v_bias torch.Size([384])\n","encoder.blocks.1.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.1.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.1.attn.proj.bias torch.Size([384])\n","encoder.blocks.1.norm2.weight torch.Size([384])\n","encoder.blocks.1.norm2.bias torch.Size([384])\n","encoder.blocks.1.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.1.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.1.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.1.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.2.norm1.weight torch.Size([384])\n","encoder.blocks.2.norm1.bias torch.Size([384])\n","encoder.blocks.2.attn.q_bias torch.Size([384])\n","encoder.blocks.2.attn.v_bias torch.Size([384])\n","encoder.blocks.2.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.2.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.2.attn.proj.bias torch.Size([384])\n","encoder.blocks.2.norm2.weight torch.Size([384])\n","encoder.blocks.2.norm2.bias torch.Size([384])\n","encoder.blocks.2.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.2.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.2.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.2.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.3.norm1.weight torch.Size([384])\n","encoder.blocks.3.norm1.bias torch.Size([384])\n","encoder.blocks.3.attn.q_bias torch.Size([384])\n","encoder.blocks.3.attn.v_bias torch.Size([384])\n","encoder.blocks.3.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.3.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.3.attn.proj.bias torch.Size([384])\n","encoder.blocks.3.norm2.weight torch.Size([384])\n","encoder.blocks.3.norm2.bias torch.Size([384])\n","encoder.blocks.3.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.3.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.3.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.3.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.4.norm1.weight torch.Size([384])\n","encoder.blocks.4.norm1.bias torch.Size([384])\n","encoder.blocks.4.attn.q_bias torch.Size([384])\n","encoder.blocks.4.attn.v_bias torch.Size([384])\n","encoder.blocks.4.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.4.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.4.attn.proj.bias torch.Size([384])\n","encoder.blocks.4.norm2.weight torch.Size([384])\n","encoder.blocks.4.norm2.bias torch.Size([384])\n","encoder.blocks.4.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.4.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.4.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.4.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.5.norm1.weight torch.Size([384])\n","encoder.blocks.5.norm1.bias torch.Size([384])\n","encoder.blocks.5.attn.q_bias torch.Size([384])\n","encoder.blocks.5.attn.v_bias torch.Size([384])\n","encoder.blocks.5.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.5.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.5.attn.proj.bias torch.Size([384])\n","encoder.blocks.5.norm2.weight torch.Size([384])\n","encoder.blocks.5.norm2.bias torch.Size([384])\n","encoder.blocks.5.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.5.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.5.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.5.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.6.norm1.weight torch.Size([384])\n","encoder.blocks.6.norm1.bias torch.Size([384])\n","encoder.blocks.6.attn.q_bias torch.Size([384])\n","encoder.blocks.6.attn.v_bias torch.Size([384])\n","encoder.blocks.6.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.6.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.6.attn.proj.bias torch.Size([384])\n","encoder.blocks.6.norm2.weight torch.Size([384])\n","encoder.blocks.6.norm2.bias torch.Size([384])\n","encoder.blocks.6.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.6.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.6.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.6.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.7.norm1.weight torch.Size([384])\n","encoder.blocks.7.norm1.bias torch.Size([384])\n","encoder.blocks.7.attn.q_bias torch.Size([384])\n","encoder.blocks.7.attn.v_bias torch.Size([384])\n","encoder.blocks.7.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.7.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.7.attn.proj.bias torch.Size([384])\n","encoder.blocks.7.norm2.weight torch.Size([384])\n","encoder.blocks.7.norm2.bias torch.Size([384])\n","encoder.blocks.7.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.7.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.7.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.7.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.8.norm1.weight torch.Size([384])\n","encoder.blocks.8.norm1.bias torch.Size([384])\n","encoder.blocks.8.attn.q_bias torch.Size([384])\n","encoder.blocks.8.attn.v_bias torch.Size([384])\n","encoder.blocks.8.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.8.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.8.attn.proj.bias torch.Size([384])\n","encoder.blocks.8.norm2.weight torch.Size([384])\n","encoder.blocks.8.norm2.bias torch.Size([384])\n","encoder.blocks.8.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.8.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.8.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.8.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.9.norm1.weight torch.Size([384])\n","encoder.blocks.9.norm1.bias torch.Size([384])\n","encoder.blocks.9.attn.q_bias torch.Size([384])\n","encoder.blocks.9.attn.v_bias torch.Size([384])\n","encoder.blocks.9.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.9.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.9.attn.proj.bias torch.Size([384])\n","encoder.blocks.9.norm2.weight torch.Size([384])\n","encoder.blocks.9.norm2.bias torch.Size([384])\n","encoder.blocks.9.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.9.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.9.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.9.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.10.norm1.weight torch.Size([384])\n","encoder.blocks.10.norm1.bias torch.Size([384])\n","encoder.blocks.10.attn.q_bias torch.Size([384])\n","encoder.blocks.10.attn.v_bias torch.Size([384])\n","encoder.blocks.10.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.10.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.10.attn.proj.bias torch.Size([384])\n","encoder.blocks.10.norm2.weight torch.Size([384])\n","encoder.blocks.10.norm2.bias torch.Size([384])\n","encoder.blocks.10.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.10.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.10.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.10.mlp.fc2.bias torch.Size([384])\n","encoder.blocks.11.norm1.weight torch.Size([384])\n","encoder.blocks.11.norm1.bias torch.Size([384])\n","encoder.blocks.11.attn.q_bias torch.Size([384])\n","encoder.blocks.11.attn.v_bias torch.Size([384])\n","encoder.blocks.11.attn.qkv.weight torch.Size([1152, 384])\n","encoder.blocks.11.attn.proj.weight torch.Size([384, 384])\n","encoder.blocks.11.attn.proj.bias torch.Size([384])\n","encoder.blocks.11.norm2.weight torch.Size([384])\n","encoder.blocks.11.norm2.bias torch.Size([384])\n","encoder.blocks.11.mlp.fc1.weight torch.Size([1536, 384])\n","encoder.blocks.11.mlp.fc1.bias torch.Size([1536])\n","encoder.blocks.11.mlp.fc2.weight torch.Size([384, 1536])\n","encoder.blocks.11.mlp.fc2.bias torch.Size([384])\n","encoder.norm.weight torch.Size([384])\n","encoder.norm.bias torch.Size([384])\n","decoder.blocks.0.norm1.weight torch.Size([192])\n","decoder.blocks.0.norm1.bias torch.Size([192])\n","decoder.blocks.0.attn.q_bias torch.Size([192])\n","decoder.blocks.0.attn.v_bias torch.Size([192])\n","decoder.blocks.0.attn.qkv.weight torch.Size([576, 192])\n","decoder.blocks.0.attn.proj.weight torch.Size([192, 192])\n","decoder.blocks.0.attn.proj.bias torch.Size([192])\n","decoder.blocks.0.norm2.weight torch.Size([192])\n","decoder.blocks.0.norm2.bias torch.Size([192])\n","decoder.blocks.0.mlp.fc1.weight torch.Size([768, 192])\n","decoder.blocks.0.mlp.fc1.bias torch.Size([768])\n","decoder.blocks.0.mlp.fc2.weight torch.Size([192, 768])\n","decoder.blocks.0.mlp.fc2.bias torch.Size([192])\n","decoder.blocks.1.norm1.weight torch.Size([192])\n","decoder.blocks.1.norm1.bias torch.Size([192])\n","decoder.blocks.1.attn.q_bias torch.Size([192])\n","decoder.blocks.1.attn.v_bias torch.Size([192])\n","decoder.blocks.1.attn.qkv.weight torch.Size([576, 192])\n","decoder.blocks.1.attn.proj.weight torch.Size([192, 192])\n","decoder.blocks.1.attn.proj.bias torch.Size([192])\n","decoder.blocks.1.norm2.weight torch.Size([192])\n","decoder.blocks.1.norm2.bias torch.Size([192])\n","decoder.blocks.1.mlp.fc1.weight torch.Size([768, 192])\n","decoder.blocks.1.mlp.fc1.bias torch.Size([768])\n","decoder.blocks.1.mlp.fc2.weight torch.Size([192, 768])\n","decoder.blocks.1.mlp.fc2.bias torch.Size([192])\n","decoder.blocks.2.norm1.weight torch.Size([192])\n","decoder.blocks.2.norm1.bias torch.Size([192])\n","decoder.blocks.2.attn.q_bias torch.Size([192])\n","decoder.blocks.2.attn.v_bias torch.Size([192])\n","decoder.blocks.2.attn.qkv.weight torch.Size([576, 192])\n","decoder.blocks.2.attn.proj.weight torch.Size([192, 192])\n","decoder.blocks.2.attn.proj.bias torch.Size([192])\n","decoder.blocks.2.norm2.weight torch.Size([192])\n","decoder.blocks.2.norm2.bias torch.Size([192])\n","decoder.blocks.2.mlp.fc1.weight torch.Size([768, 192])\n","decoder.blocks.2.mlp.fc1.bias torch.Size([768])\n","decoder.blocks.2.mlp.fc2.weight torch.Size([192, 768])\n","decoder.blocks.2.mlp.fc2.bias torch.Size([192])\n","decoder.blocks.3.norm1.weight torch.Size([192])\n","decoder.blocks.3.norm1.bias torch.Size([192])\n","decoder.blocks.3.attn.q_bias torch.Size([192])\n","decoder.blocks.3.attn.v_bias torch.Size([192])\n","decoder.blocks.3.attn.qkv.weight torch.Size([576, 192])\n","decoder.blocks.3.attn.proj.weight torch.Size([192, 192])\n","decoder.blocks.3.attn.proj.bias torch.Size([192])\n","decoder.blocks.3.norm2.weight torch.Size([192])\n","decoder.blocks.3.norm2.bias torch.Size([192])\n","decoder.blocks.3.mlp.fc1.weight torch.Size([768, 192])\n","decoder.blocks.3.mlp.fc1.bias torch.Size([768])\n","decoder.blocks.3.mlp.fc2.weight torch.Size([192, 768])\n","decoder.blocks.3.mlp.fc2.bias torch.Size([192])\n","decoder.norm.weight torch.Size([192])\n","decoder.norm.bias torch.Size([192])\n","decoder.head.weight torch.Size([768, 192])\n","decoder.head.bias torch.Size([768])\n","encoder_to_decoder.weight torch.Size([192, 384])\n","=============================================================================================\n","Total number of parameters: 23586240\n"]}],"source":["model = pretrain_mae_small_patch16_224() \n","print_parameters(model) "]},{"cell_type":"markdown","metadata":{},"source":["### mae base"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:15.805146Z","iopub.status.busy":"2024-05-09T01:00:15.804408Z","iopub.status.idle":"2024-05-09T01:00:15.811639Z","shell.execute_reply":"2024-05-09T01:00:15.810299Z","shell.execute_reply.started":"2024-05-09T01:00:15.805112Z"},"trusted":true},"outputs":[],"source":["# @register_model\n","def pretrain_mae_base_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=768, \n","        encoder_depth=12, \n","        encoder_num_heads=12,\n","        encoder_num_classes=0,\n","        decoder_num_classes=768,\n","        decoder_embed_dim=384,\n","        decoder_depth=4,\n","        decoder_num_heads=6,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:15.814094Z","iopub.status.busy":"2024-05-09T01:00:15.813192Z","iopub.status.idle":"2024-05-09T01:00:18.157894Z","shell.execute_reply":"2024-05-09T01:00:18.156845Z","shell.execute_reply.started":"2024-05-09T01:00:15.814064Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mask_token torch.Size([1, 1, 384])\n","encoder.patch_embed.proj.weight torch.Size([768, 3, 16, 16])\n","encoder.patch_embed.proj.bias torch.Size([768])\n","encoder.blocks.0.norm1.weight torch.Size([768])\n","encoder.blocks.0.norm1.bias torch.Size([768])\n","encoder.blocks.0.attn.q_bias torch.Size([768])\n","encoder.blocks.0.attn.v_bias torch.Size([768])\n","encoder.blocks.0.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.0.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.0.attn.proj.bias torch.Size([768])\n","encoder.blocks.0.norm2.weight torch.Size([768])\n","encoder.blocks.0.norm2.bias torch.Size([768])\n","encoder.blocks.0.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.0.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.0.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.0.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.1.norm1.weight torch.Size([768])\n","encoder.blocks.1.norm1.bias torch.Size([768])\n","encoder.blocks.1.attn.q_bias torch.Size([768])\n","encoder.blocks.1.attn.v_bias torch.Size([768])\n","encoder.blocks.1.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.1.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.1.attn.proj.bias torch.Size([768])\n","encoder.blocks.1.norm2.weight torch.Size([768])\n","encoder.blocks.1.norm2.bias torch.Size([768])\n","encoder.blocks.1.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.1.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.1.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.1.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.2.norm1.weight torch.Size([768])\n","encoder.blocks.2.norm1.bias torch.Size([768])\n","encoder.blocks.2.attn.q_bias torch.Size([768])\n","encoder.blocks.2.attn.v_bias torch.Size([768])\n","encoder.blocks.2.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.2.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.2.attn.proj.bias torch.Size([768])\n","encoder.blocks.2.norm2.weight torch.Size([768])\n","encoder.blocks.2.norm2.bias torch.Size([768])\n","encoder.blocks.2.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.2.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.2.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.2.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.3.norm1.weight torch.Size([768])\n","encoder.blocks.3.norm1.bias torch.Size([768])\n","encoder.blocks.3.attn.q_bias torch.Size([768])\n","encoder.blocks.3.attn.v_bias torch.Size([768])\n","encoder.blocks.3.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.3.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.3.attn.proj.bias torch.Size([768])\n","encoder.blocks.3.norm2.weight torch.Size([768])\n","encoder.blocks.3.norm2.bias torch.Size([768])\n","encoder.blocks.3.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.3.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.3.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.3.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.4.norm1.weight torch.Size([768])\n","encoder.blocks.4.norm1.bias torch.Size([768])\n","encoder.blocks.4.attn.q_bias torch.Size([768])\n","encoder.blocks.4.attn.v_bias torch.Size([768])\n","encoder.blocks.4.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.4.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.4.attn.proj.bias torch.Size([768])\n","encoder.blocks.4.norm2.weight torch.Size([768])\n","encoder.blocks.4.norm2.bias torch.Size([768])\n","encoder.blocks.4.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.4.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.4.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.4.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.5.norm1.weight torch.Size([768])\n","encoder.blocks.5.norm1.bias torch.Size([768])\n","encoder.blocks.5.attn.q_bias torch.Size([768])\n","encoder.blocks.5.attn.v_bias torch.Size([768])\n","encoder.blocks.5.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.5.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.5.attn.proj.bias torch.Size([768])\n","encoder.blocks.5.norm2.weight torch.Size([768])\n","encoder.blocks.5.norm2.bias torch.Size([768])\n","encoder.blocks.5.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.5.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.5.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.5.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.6.norm1.weight torch.Size([768])\n","encoder.blocks.6.norm1.bias torch.Size([768])\n","encoder.blocks.6.attn.q_bias torch.Size([768])\n","encoder.blocks.6.attn.v_bias torch.Size([768])\n","encoder.blocks.6.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.6.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.6.attn.proj.bias torch.Size([768])\n","encoder.blocks.6.norm2.weight torch.Size([768])\n","encoder.blocks.6.norm2.bias torch.Size([768])\n","encoder.blocks.6.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.6.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.6.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.6.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.7.norm1.weight torch.Size([768])\n","encoder.blocks.7.norm1.bias torch.Size([768])\n","encoder.blocks.7.attn.q_bias torch.Size([768])\n","encoder.blocks.7.attn.v_bias torch.Size([768])\n","encoder.blocks.7.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.7.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.7.attn.proj.bias torch.Size([768])\n","encoder.blocks.7.norm2.weight torch.Size([768])\n","encoder.blocks.7.norm2.bias torch.Size([768])\n","encoder.blocks.7.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.7.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.7.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.7.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.8.norm1.weight torch.Size([768])\n","encoder.blocks.8.norm1.bias torch.Size([768])\n","encoder.blocks.8.attn.q_bias torch.Size([768])\n","encoder.blocks.8.attn.v_bias torch.Size([768])\n","encoder.blocks.8.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.8.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.8.attn.proj.bias torch.Size([768])\n","encoder.blocks.8.norm2.weight torch.Size([768])\n","encoder.blocks.8.norm2.bias torch.Size([768])\n","encoder.blocks.8.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.8.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.8.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.8.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.9.norm1.weight torch.Size([768])\n","encoder.blocks.9.norm1.bias torch.Size([768])\n","encoder.blocks.9.attn.q_bias torch.Size([768])\n","encoder.blocks.9.attn.v_bias torch.Size([768])\n","encoder.blocks.9.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.9.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.9.attn.proj.bias torch.Size([768])\n","encoder.blocks.9.norm2.weight torch.Size([768])\n","encoder.blocks.9.norm2.bias torch.Size([768])\n","encoder.blocks.9.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.9.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.9.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.9.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.10.norm1.weight torch.Size([768])\n","encoder.blocks.10.norm1.bias torch.Size([768])\n","encoder.blocks.10.attn.q_bias torch.Size([768])\n","encoder.blocks.10.attn.v_bias torch.Size([768])\n","encoder.blocks.10.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.10.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.10.attn.proj.bias torch.Size([768])\n","encoder.blocks.10.norm2.weight torch.Size([768])\n","encoder.blocks.10.norm2.bias torch.Size([768])\n","encoder.blocks.10.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.10.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.10.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.10.mlp.fc2.bias torch.Size([768])\n","encoder.blocks.11.norm1.weight torch.Size([768])\n","encoder.blocks.11.norm1.bias torch.Size([768])\n","encoder.blocks.11.attn.q_bias torch.Size([768])\n","encoder.blocks.11.attn.v_bias torch.Size([768])\n","encoder.blocks.11.attn.qkv.weight torch.Size([2304, 768])\n","encoder.blocks.11.attn.proj.weight torch.Size([768, 768])\n","encoder.blocks.11.attn.proj.bias torch.Size([768])\n","encoder.blocks.11.norm2.weight torch.Size([768])\n","encoder.blocks.11.norm2.bias torch.Size([768])\n","encoder.blocks.11.mlp.fc1.weight torch.Size([3072, 768])\n","encoder.blocks.11.mlp.fc1.bias torch.Size([3072])\n","encoder.blocks.11.mlp.fc2.weight torch.Size([768, 3072])\n","encoder.blocks.11.mlp.fc2.bias torch.Size([768])\n","encoder.norm.weight torch.Size([768])\n","encoder.norm.bias torch.Size([768])\n","decoder.blocks.0.norm1.weight torch.Size([384])\n","decoder.blocks.0.norm1.bias torch.Size([384])\n","decoder.blocks.0.attn.q_bias torch.Size([384])\n","decoder.blocks.0.attn.v_bias torch.Size([384])\n","decoder.blocks.0.attn.qkv.weight torch.Size([1152, 384])\n","decoder.blocks.0.attn.proj.weight torch.Size([384, 384])\n","decoder.blocks.0.attn.proj.bias torch.Size([384])\n","decoder.blocks.0.norm2.weight torch.Size([384])\n","decoder.blocks.0.norm2.bias torch.Size([384])\n","decoder.blocks.0.mlp.fc1.weight torch.Size([1536, 384])\n","decoder.blocks.0.mlp.fc1.bias torch.Size([1536])\n","decoder.blocks.0.mlp.fc2.weight torch.Size([384, 1536])\n","decoder.blocks.0.mlp.fc2.bias torch.Size([384])\n","decoder.blocks.1.norm1.weight torch.Size([384])\n","decoder.blocks.1.norm1.bias torch.Size([384])\n","decoder.blocks.1.attn.q_bias torch.Size([384])\n","decoder.blocks.1.attn.v_bias torch.Size([384])\n","decoder.blocks.1.attn.qkv.weight torch.Size([1152, 384])\n","decoder.blocks.1.attn.proj.weight torch.Size([384, 384])\n","decoder.blocks.1.attn.proj.bias torch.Size([384])\n","decoder.blocks.1.norm2.weight torch.Size([384])\n","decoder.blocks.1.norm2.bias torch.Size([384])\n","decoder.blocks.1.mlp.fc1.weight torch.Size([1536, 384])\n","decoder.blocks.1.mlp.fc1.bias torch.Size([1536])\n","decoder.blocks.1.mlp.fc2.weight torch.Size([384, 1536])\n","decoder.blocks.1.mlp.fc2.bias torch.Size([384])\n","decoder.blocks.2.norm1.weight torch.Size([384])\n","decoder.blocks.2.norm1.bias torch.Size([384])\n","decoder.blocks.2.attn.q_bias torch.Size([384])\n","decoder.blocks.2.attn.v_bias torch.Size([384])\n","decoder.blocks.2.attn.qkv.weight torch.Size([1152, 384])\n","decoder.blocks.2.attn.proj.weight torch.Size([384, 384])\n","decoder.blocks.2.attn.proj.bias torch.Size([384])\n","decoder.blocks.2.norm2.weight torch.Size([384])\n","decoder.blocks.2.norm2.bias torch.Size([384])\n","decoder.blocks.2.mlp.fc1.weight torch.Size([1536, 384])\n","decoder.blocks.2.mlp.fc1.bias torch.Size([1536])\n","decoder.blocks.2.mlp.fc2.weight torch.Size([384, 1536])\n","decoder.blocks.2.mlp.fc2.bias torch.Size([384])\n","decoder.blocks.3.norm1.weight torch.Size([384])\n","decoder.blocks.3.norm1.bias torch.Size([384])\n","decoder.blocks.3.attn.q_bias torch.Size([384])\n","decoder.blocks.3.attn.v_bias torch.Size([384])\n","decoder.blocks.3.attn.qkv.weight torch.Size([1152, 384])\n","decoder.blocks.3.attn.proj.weight torch.Size([384, 384])\n","decoder.blocks.3.attn.proj.bias torch.Size([384])\n","decoder.blocks.3.norm2.weight torch.Size([384])\n","decoder.blocks.3.norm2.bias torch.Size([384])\n","decoder.blocks.3.mlp.fc1.weight torch.Size([1536, 384])\n","decoder.blocks.3.mlp.fc1.bias torch.Size([1536])\n","decoder.blocks.3.mlp.fc2.weight torch.Size([384, 1536])\n","decoder.blocks.3.mlp.fc2.bias torch.Size([384])\n","decoder.norm.weight torch.Size([384])\n","decoder.norm.bias torch.Size([384])\n","decoder.head.weight torch.Size([768, 384])\n","decoder.head.bias torch.Size([768])\n","encoder_to_decoder.weight torch.Size([384, 768])\n","=============================================================================================\n","Total number of parameters: 93325440\n"]}],"source":["model = pretrain_mae_base_patch16_224()  \n","print_parameters(model)"]},{"cell_type":"markdown","metadata":{},"source":["### mae large"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:18.160230Z","iopub.status.busy":"2024-05-09T01:00:18.159458Z","iopub.status.idle":"2024-05-09T01:00:18.168902Z","shell.execute_reply":"2024-05-09T01:00:18.167538Z","shell.execute_reply.started":"2024-05-09T01:00:18.160192Z"},"trusted":true},"outputs":[],"source":["# @register_model\n","def pretrain_mae_large_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=1024, \n","        encoder_depth=24, \n","        encoder_num_heads=16,\n","        encoder_num_classes=0,\n","        decoder_num_classes=768,\n","        decoder_embed_dim=512,\n","        decoder_depth=8,\n","        decoder_num_heads=8,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T01:00:18.170450Z","iopub.status.busy":"2024-05-09T01:00:18.170126Z","iopub.status.idle":"2024-05-09T01:00:25.475122Z","shell.execute_reply":"2024-05-09T01:00:25.474048Z","shell.execute_reply.started":"2024-05-09T01:00:18.170423Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mask_token torch.Size([1, 1, 512])\n","encoder.patch_embed.proj.weight torch.Size([1024, 3, 16, 16])\n","encoder.patch_embed.proj.bias torch.Size([1024])\n","encoder.blocks.0.norm1.weight torch.Size([1024])\n","encoder.blocks.0.norm1.bias torch.Size([1024])\n","encoder.blocks.0.attn.q_bias torch.Size([1024])\n","encoder.blocks.0.attn.v_bias torch.Size([1024])\n","encoder.blocks.0.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.0.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.0.attn.proj.bias torch.Size([1024])\n","encoder.blocks.0.norm2.weight torch.Size([1024])\n","encoder.blocks.0.norm2.bias torch.Size([1024])\n","encoder.blocks.0.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.0.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.0.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.0.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.1.norm1.weight torch.Size([1024])\n","encoder.blocks.1.norm1.bias torch.Size([1024])\n","encoder.blocks.1.attn.q_bias torch.Size([1024])\n","encoder.blocks.1.attn.v_bias torch.Size([1024])\n","encoder.blocks.1.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.1.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.1.attn.proj.bias torch.Size([1024])\n","encoder.blocks.1.norm2.weight torch.Size([1024])\n","encoder.blocks.1.norm2.bias torch.Size([1024])\n","encoder.blocks.1.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.1.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.1.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.1.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.2.norm1.weight torch.Size([1024])\n","encoder.blocks.2.norm1.bias torch.Size([1024])\n","encoder.blocks.2.attn.q_bias torch.Size([1024])\n","encoder.blocks.2.attn.v_bias torch.Size([1024])\n","encoder.blocks.2.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.2.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.2.attn.proj.bias torch.Size([1024])\n","encoder.blocks.2.norm2.weight torch.Size([1024])\n","encoder.blocks.2.norm2.bias torch.Size([1024])\n","encoder.blocks.2.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.2.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.2.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.2.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.3.norm1.weight torch.Size([1024])\n","encoder.blocks.3.norm1.bias torch.Size([1024])\n","encoder.blocks.3.attn.q_bias torch.Size([1024])\n","encoder.blocks.3.attn.v_bias torch.Size([1024])\n","encoder.blocks.3.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.3.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.3.attn.proj.bias torch.Size([1024])\n","encoder.blocks.3.norm2.weight torch.Size([1024])\n","encoder.blocks.3.norm2.bias torch.Size([1024])\n","encoder.blocks.3.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.3.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.3.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.3.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.4.norm1.weight torch.Size([1024])\n","encoder.blocks.4.norm1.bias torch.Size([1024])\n","encoder.blocks.4.attn.q_bias torch.Size([1024])\n","encoder.blocks.4.attn.v_bias torch.Size([1024])\n","encoder.blocks.4.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.4.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.4.attn.proj.bias torch.Size([1024])\n","encoder.blocks.4.norm2.weight torch.Size([1024])\n","encoder.blocks.4.norm2.bias torch.Size([1024])\n","encoder.blocks.4.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.4.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.4.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.4.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.5.norm1.weight torch.Size([1024])\n","encoder.blocks.5.norm1.bias torch.Size([1024])\n","encoder.blocks.5.attn.q_bias torch.Size([1024])\n","encoder.blocks.5.attn.v_bias torch.Size([1024])\n","encoder.blocks.5.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.5.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.5.attn.proj.bias torch.Size([1024])\n","encoder.blocks.5.norm2.weight torch.Size([1024])\n","encoder.blocks.5.norm2.bias torch.Size([1024])\n","encoder.blocks.5.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.5.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.5.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.5.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.6.norm1.weight torch.Size([1024])\n","encoder.blocks.6.norm1.bias torch.Size([1024])\n","encoder.blocks.6.attn.q_bias torch.Size([1024])\n","encoder.blocks.6.attn.v_bias torch.Size([1024])\n","encoder.blocks.6.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.6.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.6.attn.proj.bias torch.Size([1024])\n","encoder.blocks.6.norm2.weight torch.Size([1024])\n","encoder.blocks.6.norm2.bias torch.Size([1024])\n","encoder.blocks.6.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.6.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.6.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.6.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.7.norm1.weight torch.Size([1024])\n","encoder.blocks.7.norm1.bias torch.Size([1024])\n","encoder.blocks.7.attn.q_bias torch.Size([1024])\n","encoder.blocks.7.attn.v_bias torch.Size([1024])\n","encoder.blocks.7.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.7.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.7.attn.proj.bias torch.Size([1024])\n","encoder.blocks.7.norm2.weight torch.Size([1024])\n","encoder.blocks.7.norm2.bias torch.Size([1024])\n","encoder.blocks.7.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.7.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.7.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.7.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.8.norm1.weight torch.Size([1024])\n","encoder.blocks.8.norm1.bias torch.Size([1024])\n","encoder.blocks.8.attn.q_bias torch.Size([1024])\n","encoder.blocks.8.attn.v_bias torch.Size([1024])\n","encoder.blocks.8.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.8.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.8.attn.proj.bias torch.Size([1024])\n","encoder.blocks.8.norm2.weight torch.Size([1024])\n","encoder.blocks.8.norm2.bias torch.Size([1024])\n","encoder.blocks.8.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.8.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.8.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.8.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.9.norm1.weight torch.Size([1024])\n","encoder.blocks.9.norm1.bias torch.Size([1024])\n","encoder.blocks.9.attn.q_bias torch.Size([1024])\n","encoder.blocks.9.attn.v_bias torch.Size([1024])\n","encoder.blocks.9.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.9.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.9.attn.proj.bias torch.Size([1024])\n","encoder.blocks.9.norm2.weight torch.Size([1024])\n","encoder.blocks.9.norm2.bias torch.Size([1024])\n","encoder.blocks.9.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.9.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.9.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.9.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.10.norm1.weight torch.Size([1024])\n","encoder.blocks.10.norm1.bias torch.Size([1024])\n","encoder.blocks.10.attn.q_bias torch.Size([1024])\n","encoder.blocks.10.attn.v_bias torch.Size([1024])\n","encoder.blocks.10.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.10.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.10.attn.proj.bias torch.Size([1024])\n","encoder.blocks.10.norm2.weight torch.Size([1024])\n","encoder.blocks.10.norm2.bias torch.Size([1024])\n","encoder.blocks.10.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.10.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.10.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.10.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.11.norm1.weight torch.Size([1024])\n","encoder.blocks.11.norm1.bias torch.Size([1024])\n","encoder.blocks.11.attn.q_bias torch.Size([1024])\n","encoder.blocks.11.attn.v_bias torch.Size([1024])\n","encoder.blocks.11.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.11.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.11.attn.proj.bias torch.Size([1024])\n","encoder.blocks.11.norm2.weight torch.Size([1024])\n","encoder.blocks.11.norm2.bias torch.Size([1024])\n","encoder.blocks.11.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.11.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.11.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.11.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.12.norm1.weight torch.Size([1024])\n","encoder.blocks.12.norm1.bias torch.Size([1024])\n","encoder.blocks.12.attn.q_bias torch.Size([1024])\n","encoder.blocks.12.attn.v_bias torch.Size([1024])\n","encoder.blocks.12.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.12.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.12.attn.proj.bias torch.Size([1024])\n","encoder.blocks.12.norm2.weight torch.Size([1024])\n","encoder.blocks.12.norm2.bias torch.Size([1024])\n","encoder.blocks.12.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.12.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.12.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.12.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.13.norm1.weight torch.Size([1024])\n","encoder.blocks.13.norm1.bias torch.Size([1024])\n","encoder.blocks.13.attn.q_bias torch.Size([1024])\n","encoder.blocks.13.attn.v_bias torch.Size([1024])\n","encoder.blocks.13.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.13.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.13.attn.proj.bias torch.Size([1024])\n","encoder.blocks.13.norm2.weight torch.Size([1024])\n","encoder.blocks.13.norm2.bias torch.Size([1024])\n","encoder.blocks.13.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.13.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.13.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.13.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.14.norm1.weight torch.Size([1024])\n","encoder.blocks.14.norm1.bias torch.Size([1024])\n","encoder.blocks.14.attn.q_bias torch.Size([1024])\n","encoder.blocks.14.attn.v_bias torch.Size([1024])\n","encoder.blocks.14.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.14.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.14.attn.proj.bias torch.Size([1024])\n","encoder.blocks.14.norm2.weight torch.Size([1024])\n","encoder.blocks.14.norm2.bias torch.Size([1024])\n","encoder.blocks.14.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.14.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.14.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.14.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.15.norm1.weight torch.Size([1024])\n","encoder.blocks.15.norm1.bias torch.Size([1024])\n","encoder.blocks.15.attn.q_bias torch.Size([1024])\n","encoder.blocks.15.attn.v_bias torch.Size([1024])\n","encoder.blocks.15.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.15.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.15.attn.proj.bias torch.Size([1024])\n","encoder.blocks.15.norm2.weight torch.Size([1024])\n","encoder.blocks.15.norm2.bias torch.Size([1024])\n","encoder.blocks.15.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.15.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.15.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.15.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.16.norm1.weight torch.Size([1024])\n","encoder.blocks.16.norm1.bias torch.Size([1024])\n","encoder.blocks.16.attn.q_bias torch.Size([1024])\n","encoder.blocks.16.attn.v_bias torch.Size([1024])\n","encoder.blocks.16.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.16.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.16.attn.proj.bias torch.Size([1024])\n","encoder.blocks.16.norm2.weight torch.Size([1024])\n","encoder.blocks.16.norm2.bias torch.Size([1024])\n","encoder.blocks.16.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.16.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.16.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.16.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.17.norm1.weight torch.Size([1024])\n","encoder.blocks.17.norm1.bias torch.Size([1024])\n","encoder.blocks.17.attn.q_bias torch.Size([1024])\n","encoder.blocks.17.attn.v_bias torch.Size([1024])\n","encoder.blocks.17.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.17.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.17.attn.proj.bias torch.Size([1024])\n","encoder.blocks.17.norm2.weight torch.Size([1024])\n","encoder.blocks.17.norm2.bias torch.Size([1024])\n","encoder.blocks.17.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.17.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.17.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.17.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.18.norm1.weight torch.Size([1024])\n","encoder.blocks.18.norm1.bias torch.Size([1024])\n","encoder.blocks.18.attn.q_bias torch.Size([1024])\n","encoder.blocks.18.attn.v_bias torch.Size([1024])\n","encoder.blocks.18.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.18.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.18.attn.proj.bias torch.Size([1024])\n","encoder.blocks.18.norm2.weight torch.Size([1024])\n","encoder.blocks.18.norm2.bias torch.Size([1024])\n","encoder.blocks.18.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.18.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.18.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.18.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.19.norm1.weight torch.Size([1024])\n","encoder.blocks.19.norm1.bias torch.Size([1024])\n","encoder.blocks.19.attn.q_bias torch.Size([1024])\n","encoder.blocks.19.attn.v_bias torch.Size([1024])\n","encoder.blocks.19.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.19.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.19.attn.proj.bias torch.Size([1024])\n","encoder.blocks.19.norm2.weight torch.Size([1024])\n","encoder.blocks.19.norm2.bias torch.Size([1024])\n","encoder.blocks.19.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.19.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.19.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.19.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.20.norm1.weight torch.Size([1024])\n","encoder.blocks.20.norm1.bias torch.Size([1024])\n","encoder.blocks.20.attn.q_bias torch.Size([1024])\n","encoder.blocks.20.attn.v_bias torch.Size([1024])\n","encoder.blocks.20.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.20.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.20.attn.proj.bias torch.Size([1024])\n","encoder.blocks.20.norm2.weight torch.Size([1024])\n","encoder.blocks.20.norm2.bias torch.Size([1024])\n","encoder.blocks.20.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.20.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.20.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.20.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.21.norm1.weight torch.Size([1024])\n","encoder.blocks.21.norm1.bias torch.Size([1024])\n","encoder.blocks.21.attn.q_bias torch.Size([1024])\n","encoder.blocks.21.attn.v_bias torch.Size([1024])\n","encoder.blocks.21.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.21.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.21.attn.proj.bias torch.Size([1024])\n","encoder.blocks.21.norm2.weight torch.Size([1024])\n","encoder.blocks.21.norm2.bias torch.Size([1024])\n","encoder.blocks.21.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.21.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.21.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.21.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.22.norm1.weight torch.Size([1024])\n","encoder.blocks.22.norm1.bias torch.Size([1024])\n","encoder.blocks.22.attn.q_bias torch.Size([1024])\n","encoder.blocks.22.attn.v_bias torch.Size([1024])\n","encoder.blocks.22.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.22.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.22.attn.proj.bias torch.Size([1024])\n","encoder.blocks.22.norm2.weight torch.Size([1024])\n","encoder.blocks.22.norm2.bias torch.Size([1024])\n","encoder.blocks.22.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.22.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.22.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.22.mlp.fc2.bias torch.Size([1024])\n","encoder.blocks.23.norm1.weight torch.Size([1024])\n","encoder.blocks.23.norm1.bias torch.Size([1024])\n","encoder.blocks.23.attn.q_bias torch.Size([1024])\n","encoder.blocks.23.attn.v_bias torch.Size([1024])\n","encoder.blocks.23.attn.qkv.weight torch.Size([3072, 1024])\n","encoder.blocks.23.attn.proj.weight torch.Size([1024, 1024])\n","encoder.blocks.23.attn.proj.bias torch.Size([1024])\n","encoder.blocks.23.norm2.weight torch.Size([1024])\n","encoder.blocks.23.norm2.bias torch.Size([1024])\n","encoder.blocks.23.mlp.fc1.weight torch.Size([4096, 1024])\n","encoder.blocks.23.mlp.fc1.bias torch.Size([4096])\n","encoder.blocks.23.mlp.fc2.weight torch.Size([1024, 4096])\n","encoder.blocks.23.mlp.fc2.bias torch.Size([1024])\n","encoder.norm.weight torch.Size([1024])\n","encoder.norm.bias torch.Size([1024])\n","decoder.blocks.0.norm1.weight torch.Size([512])\n","decoder.blocks.0.norm1.bias torch.Size([512])\n","decoder.blocks.0.attn.q_bias torch.Size([512])\n","decoder.blocks.0.attn.v_bias torch.Size([512])\n","decoder.blocks.0.attn.qkv.weight torch.Size([1536, 512])\n","decoder.blocks.0.attn.proj.weight torch.Size([512, 512])\n","decoder.blocks.0.attn.proj.bias torch.Size([512])\n","decoder.blocks.0.norm2.weight torch.Size([512])\n","decoder.blocks.0.norm2.bias torch.Size([512])\n","decoder.blocks.0.mlp.fc1.weight torch.Size([2048, 512])\n","decoder.blocks.0.mlp.fc1.bias torch.Size([2048])\n","decoder.blocks.0.mlp.fc2.weight torch.Size([512, 2048])\n","decoder.blocks.0.mlp.fc2.bias torch.Size([512])\n","decoder.blocks.1.norm1.weight torch.Size([512])\n","decoder.blocks.1.norm1.bias torch.Size([512])\n","decoder.blocks.1.attn.q_bias torch.Size([512])\n","decoder.blocks.1.attn.v_bias torch.Size([512])\n","decoder.blocks.1.attn.qkv.weight torch.Size([1536, 512])\n","decoder.blocks.1.attn.proj.weight torch.Size([512, 512])\n","decoder.blocks.1.attn.proj.bias torch.Size([512])\n","decoder.blocks.1.norm2.weight torch.Size([512])\n","decoder.blocks.1.norm2.bias torch.Size([512])\n","decoder.blocks.1.mlp.fc1.weight torch.Size([2048, 512])\n","decoder.blocks.1.mlp.fc1.bias torch.Size([2048])\n","decoder.blocks.1.mlp.fc2.weight torch.Size([512, 2048])\n","decoder.blocks.1.mlp.fc2.bias torch.Size([512])\n","decoder.blocks.2.norm1.weight torch.Size([512])\n","decoder.blocks.2.norm1.bias torch.Size([512])\n","decoder.blocks.2.attn.q_bias torch.Size([512])\n","decoder.blocks.2.attn.v_bias torch.Size([512])\n","decoder.blocks.2.attn.qkv.weight torch.Size([1536, 512])\n","decoder.blocks.2.attn.proj.weight torch.Size([512, 512])\n","decoder.blocks.2.attn.proj.bias torch.Size([512])\n","decoder.blocks.2.norm2.weight torch.Size([512])\n","decoder.blocks.2.norm2.bias torch.Size([512])\n","decoder.blocks.2.mlp.fc1.weight torch.Size([2048, 512])\n","decoder.blocks.2.mlp.fc1.bias torch.Size([2048])\n","decoder.blocks.2.mlp.fc2.weight torch.Size([512, 2048])\n","decoder.blocks.2.mlp.fc2.bias torch.Size([512])\n","decoder.blocks.3.norm1.weight torch.Size([512])\n","decoder.blocks.3.norm1.bias torch.Size([512])\n","decoder.blocks.3.attn.q_bias torch.Size([512])\n","decoder.blocks.3.attn.v_bias torch.Size([512])\n","decoder.blocks.3.attn.qkv.weight torch.Size([1536, 512])\n","decoder.blocks.3.attn.proj.weight torch.Size([512, 512])\n","decoder.blocks.3.attn.proj.bias torch.Size([512])\n","decoder.blocks.3.norm2.weight torch.Size([512])\n","decoder.blocks.3.norm2.bias torch.Size([512])\n","decoder.blocks.3.mlp.fc1.weight torch.Size([2048, 512])\n","decoder.blocks.3.mlp.fc1.bias torch.Size([2048])\n","decoder.blocks.3.mlp.fc2.weight torch.Size([512, 2048])\n","decoder.blocks.3.mlp.fc2.bias torch.Size([512])\n","decoder.blocks.4.norm1.weight torch.Size([512])\n","decoder.blocks.4.norm1.bias torch.Size([512])\n","decoder.blocks.4.attn.q_bias torch.Size([512])\n","decoder.blocks.4.attn.v_bias torch.Size([512])\n","decoder.blocks.4.attn.qkv.weight torch.Size([1536, 512])\n","decoder.blocks.4.attn.proj.weight torch.Size([512, 512])\n","decoder.blocks.4.attn.proj.bias torch.Size([512])\n","decoder.blocks.4.norm2.weight torch.Size([512])\n","decoder.blocks.4.norm2.bias torch.Size([512])\n","decoder.blocks.4.mlp.fc1.weight torch.Size([2048, 512])\n","decoder.blocks.4.mlp.fc1.bias torch.Size([2048])\n","decoder.blocks.4.mlp.fc2.weight torch.Size([512, 2048])\n","decoder.blocks.4.mlp.fc2.bias torch.Size([512])\n","decoder.blocks.5.norm1.weight torch.Size([512])\n","decoder.blocks.5.norm1.bias torch.Size([512])\n","decoder.blocks.5.attn.q_bias torch.Size([512])\n","decoder.blocks.5.attn.v_bias torch.Size([512])\n","decoder.blocks.5.attn.qkv.weight torch.Size([1536, 512])\n","decoder.blocks.5.attn.proj.weight torch.Size([512, 512])\n","decoder.blocks.5.attn.proj.bias torch.Size([512])\n","decoder.blocks.5.norm2.weight torch.Size([512])\n","decoder.blocks.5.norm2.bias torch.Size([512])\n","decoder.blocks.5.mlp.fc1.weight torch.Size([2048, 512])\n","decoder.blocks.5.mlp.fc1.bias torch.Size([2048])\n","decoder.blocks.5.mlp.fc2.weight torch.Size([512, 2048])\n","decoder.blocks.5.mlp.fc2.bias torch.Size([512])\n","decoder.blocks.6.norm1.weight torch.Size([512])\n","decoder.blocks.6.norm1.bias torch.Size([512])\n","decoder.blocks.6.attn.q_bias torch.Size([512])\n","decoder.blocks.6.attn.v_bias torch.Size([512])\n","decoder.blocks.6.attn.qkv.weight torch.Size([1536, 512])\n","decoder.blocks.6.attn.proj.weight torch.Size([512, 512])\n","decoder.blocks.6.attn.proj.bias torch.Size([512])\n","decoder.blocks.6.norm2.weight torch.Size([512])\n","decoder.blocks.6.norm2.bias torch.Size([512])\n","decoder.blocks.6.mlp.fc1.weight torch.Size([2048, 512])\n","decoder.blocks.6.mlp.fc1.bias torch.Size([2048])\n","decoder.blocks.6.mlp.fc2.weight torch.Size([512, 2048])\n","decoder.blocks.6.mlp.fc2.bias torch.Size([512])\n","decoder.blocks.7.norm1.weight torch.Size([512])\n","decoder.blocks.7.norm1.bias torch.Size([512])\n","decoder.blocks.7.attn.q_bias torch.Size([512])\n","decoder.blocks.7.attn.v_bias torch.Size([512])\n","decoder.blocks.7.attn.qkv.weight torch.Size([1536, 512])\n","decoder.blocks.7.attn.proj.weight torch.Size([512, 512])\n","decoder.blocks.7.attn.proj.bias torch.Size([512])\n","decoder.blocks.7.norm2.weight torch.Size([512])\n","decoder.blocks.7.norm2.bias torch.Size([512])\n","decoder.blocks.7.mlp.fc1.weight torch.Size([2048, 512])\n","decoder.blocks.7.mlp.fc1.bias torch.Size([2048])\n","decoder.blocks.7.mlp.fc2.weight torch.Size([512, 2048])\n","decoder.blocks.7.mlp.fc2.bias torch.Size([512])\n","decoder.norm.weight torch.Size([512])\n","decoder.norm.bias torch.Size([512])\n","decoder.head.weight torch.Size([768, 512])\n","decoder.head.bias torch.Size([768])\n","encoder_to_decoder.weight torch.Size([512, 1024])\n","=============================================================================================\n","Total number of parameters: 329209088\n"]}],"source":["model = pretrain_mae_large_patch16_224(pretrained=False) \n","print_parameters(model)  "]},{"cell_type":"markdown","metadata":{},"source":["# Masking Generator for training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class RandomMaskingGenerator:\n","    def __init__(self, input_size, mask_ratio):\n","        if not isinstance(input_size, tuple):\n","            input_size = (input_size,) * 2\n","\n","        self.height, self.width = input_size\n","\n","        self.num_patches = self.height * self.width\n","        self.num_mask = int(mask_ratio * self.num_patches)\n","\n","    def __repr__(self):\n","        repr_str = \"Maks: total patches {}, mask patches {}\".format(\n","            self.num_patches, self.num_mask\n","        )\n","        return repr_str\n","\n","    def __call__(self):\n","        mask = np.hstack([\n","            np.zeros(self.num_patches - self.num_mask),\n","            np.ones(self.num_mask),\n","        ])\n","        np.random.shuffle(mask)\n","        return mask # [196]"]},{"cell_type":"markdown","metadata":{},"source":["### Data Augmentation"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T10:20:25.599814Z","iopub.status.busy":"2024-05-09T10:20:25.599380Z","iopub.status.idle":"2024-05-09T10:20:25.649038Z","shell.execute_reply":"2024-05-09T10:20:25.647793Z","shell.execute_reply.started":"2024-05-09T10:20:25.599763Z"},"trusted":true},"outputs":[],"source":["class DataAugmentationForMAE(object):\n","    def __init__(self, args):\n","        imagenet_default_mean_and_std = args.imagenet_default_mean_and_std\n","        mean = IMAGENET_INCEPTION_MEAN if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_MEAN\n","        std = IMAGENET_INCEPTION_STD if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_STD\n","\n","        self.transform = transforms.Compose([\n","            transforms.RandomResizedCrop(args.input_size),\n","            transforms.ToTensor(),\n","            transforms.Normalize(\n","                mean=torch.tensor(mean),\n","                std=torch.tensor(std))\n","        ])\n","\n","        self.masked_position_generator = RandomMaskingGenerator(\n","            args.window_size, args.mask_ratio\n","        )\n","\n","    def __call__(self, image):\n","        return self.transform(image), self.masked_position_generator()\n","\n","    def __repr__(self):\n","        repr = \"(DataAugmentationForBEiT,\\n\"\n","        repr += \"  transform = %s,\\n\" % str(self.transform)\n","        repr += \"  Masked position generator = %s,\\n\" % str(self.masked_position_generator)\n","        repr += \")\"\n","        return repr\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T10:20:28.810396Z","iopub.status.busy":"2024-05-09T10:20:28.810015Z","iopub.status.idle":"2024-05-09T10:20:28.816445Z","shell.execute_reply":"2024-05-09T10:20:28.814980Z","shell.execute_reply.started":"2024-05-09T10:20:28.810366Z"},"trusted":true},"outputs":[],"source":["def build_pretraining_dataset(args):\n","    transform = DataAugmentationForMAE(args)\n","    print(\"Data Aug = %s\" % str(transform))\n","    return ImageFolder(args.data_path, transform=transform)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4955202,"sourceId":8342586,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
