{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import keras\nfrom keras_cv.layers import DropPath\nfrom keras import ops\nfrom tensorflow.keras import *\nfrom tensorflow.keras.layers import *\nimport tensorflow as tf  \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.initializers import RandomNormal\nfrom functools import partial\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-09T10:10:34.127809Z","iopub.execute_input":"2024-05-09T10:10:34.128611Z","iopub.status.idle":"2024-05-09T10:10:50.529364Z","shell.execute_reply.started":"2024-05-09T10:10:34.128580Z","shell.execute_reply":"2024-05-09T10:10:50.528588Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-05-09 10:10:35.662603: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-09 10:10:35.662717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-09 10:10:35.771599: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class PatchEmbed(layers.Layer):\n    \"\"\"Patch embedding block.\n\n    Args:\n        embed_dim: feature size dimension.\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = (img_size, img_size)  \n        patch_size = (patch_size, patch_size)  \n        num_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        \n#         applying filters to the input images\n# \n        self.proj = Conv2D(filters=embed_dim,\n                                           kernel_size=patch_size,\n                                           strides=patch_size,\n                                           padding='valid')\n\n    def call(self, inputs, **kwargs):\n        B, H, W, C = inputs.shape # batch size, height,width,channel\n        assert (H, W) == self.img_size, f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        \n#         convert the input image patches to embdeddings\n        x = self.proj(inputs)\n        x = tf.reshape(x, [B, self.num_patches, -1])  # reshpate to [batch_size, num_patch, embed_dim]\n        x = tf.transpose(x, perm=[0, 2, 1])  # Transpose to match PyTorch's output shape\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.531203Z","iopub.execute_input":"2024-05-09T10:10:50.531688Z","iopub.status.idle":"2024-05-09T10:10:50.541536Z","shell.execute_reply.started":"2024-05-09T10:10:50.531662Z","shell.execute_reply":"2024-05-09T10:10:50.540589Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def get_sinusoid_encoding_table(n_position, d_hid): \n    ''' Sinusoid position encoding table ''' \n\n    def get_position_angle_vec(position): \n        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] \n\n    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)]) \n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i \n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1 \n    \n    \n#     convert the np_array to float32\n    tf_sinusoid_table = tf.convert_to_tensor(sinusoid_table, dtype=tf.float32)\n#     add a new dimension\n    tf_sinusoid_table = tf.expand_dims(tf_sinusoid_table, axis=0)\n    \n    \n    return tf_sinusoid_table","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.542627Z","iopub.execute_input":"2024-05-09T10:10:50.542983Z","iopub.status.idle":"2024-05-09T10:10:50.560338Z","shell.execute_reply.started":"2024-05-09T10:10:50.542950Z","shell.execute_reply":"2024-05-09T10:10:50.559441Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        **kwargs\n    }","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.561517Z","iopub.execute_input":"2024-05-09T10:10:50.561806Z","iopub.status.idle":"2024-05-09T10:10:50.571086Z","shell.execute_reply.started":"2024-05-09T10:10:50.561782Z","shell.execute_reply":"2024-05-09T10:10:50.570322Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Mlp(layers.Layer):\n    def __init__(self, in_features, hidden_features=None, out_features=None, \n                 act_layer=tf.nn.gelu, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        \n        self.fc1 = layers.Dense(hidden_features)\n        self.activation = act_layer\n        self.fc2 = layers.Dense(out_features)\n        self.drop = layers.Dropout(drop)  # layers.Dropout for dropout\n\n    def call(self, x, training=None):\n        x = self.fc1(x)\n        x = self.activation(x)\n        x = self.fc2(x)\n        x = self.drop(x, training=training)  #  self.drop for dropout\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.574283Z","iopub.execute_input":"2024-05-09T10:10:50.574702Z","iopub.status.idle":"2024-05-09T10:10:50.581677Z","shell.execute_reply.started":"2024-05-09T10:10:50.574678Z","shell.execute_reply":"2024-05-09T10:10:50.580739Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Attention(layers.Layer):\n    \n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., \n                 proj_drop=0., attn_head_dim=None):\n        super().__init__()\n        \n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n        \n        self.qkv = layers.Dense(all_head_dim * 3, use_bias=False)\n        \n        if qkv_bias:\n            self.q_bias = self.add_weight(shape=(all_head_dim,), initializer=\"zeros\", trainable=True)\n            self.v_bias = self.add_weight(shape=(all_head_dim,), initializer=\"zeros\", trainable=True)\n        else:\n            self.q_bias = None\n            self.v_bias = None\n            \n        self.attn_drop = layers.Dropout(attn_drop)\n        self.proj = layers.Dense(units=dim, input_dim=all_head_dim)\n        self.proj_drop = layers.Dropout(proj_drop)\n\n    def call(self, x, training=None):\n        B, N, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n        qkv_bias = None\n        \n        if self.q_bias is not None:\n            qkv_bias = tf.concat([self.q_bias, tf.zeros_like(self.v_bias)], axis=0)\n\n        qkv = tf.matmul(x, self.qkv.weight)\n        qkv = tf.nn.bias_add(qkv, qkv_bias)\n        qkv = tf.reshape(qkv, (B, N, 3, self.num_heads, -1))\n        qkv = tf.transpose(qkv, perm=[2, 0, 3, 1, 4])\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        q = q * self.scale\n        attn = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2]))\n\n        attn = tf.nn.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn, training=training)\n\n        x = tf.matmul(attn, v)\n        x = tf.transpose(x, perm=[0, 2, 1, 3])\n        x = tf.reshape(x, (B, N, -1))\n\n        x = self.proj(x)\n        x = self.proj_drop(x, training=training)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.582762Z","iopub.execute_input":"2024-05-09T10:10:50.583044Z","iopub.status.idle":"2024-05-09T10:10:50.599002Z","shell.execute_reply.started":"2024-05-09T10:10:50.583020Z","shell.execute_reply":"2024-05-09T10:10:50.598068Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class DropPath(layers.Layer):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n    \n    def extra_repr(self) -> str:\n        return 'p={}'.format(self.drop_prob)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.600306Z","iopub.execute_input":"2024-05-09T10:10:50.600639Z","iopub.status.idle":"2024-05-09T10:10:50.609510Z","shell.execute_reply.started":"2024-05-09T10:10:50.600614Z","shell.execute_reply":"2024-05-09T10:10:50.608616Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Block(layers.Layer):\n#     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n#                  drop_path=0., init_values=None, act_layer=tf.nn.gelu, norm_layer=LayerNormalization, attn_head_dim=None):\n    def __init__(self, dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path, init_values, act_layer, norm_layer, attn_head_dim):\n\n        super().__init__()\n        \n#         self.norm1 = norm_layer(epsilon=1e-6)\n#         self.attn = Attention(\n#             dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n#             attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n        \n#         self.drop_path = tf.keras.layers.Dropout(rate=drop_path)\n#         self.norm2 = norm_layer(epsilon=1e-6)\n#         mlp_hidden_dim = int(dim * mlp_ratio)\n#         self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n#         if init_values is not None:\n#             self.gamma_1 = self.add_weight(shape=(dim,), initializer=tf.constant_initializer(init_values), trainable=True)\n#             self.gamma_2 = self.add_weight(shape=(dim,), initializer=tf.constant_initializer(init_values), trainable=True)\n#         else:\n#             self.gamma_1, self.gamma_2 = None, None\n        self.norm1 = norm_layer(epsilon=1e-6)\n        self.attn = Attention(\n            use_scale=qk_scale,\n            dropout=attn_drop,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            head_dim=attn_head_dim,  # Example attention head dimension\n            name=\"attention\"\n        )\n        self.drop_path = tf.keras.layers.Dropout(rate=drop_path)\n        self.norm2 = norm_layer(epsilon=1e-6)\n\n    def call(self, inputs, **kwargs):\n        x = inputs\n        x = self.norm1(x)\n        x = self.attn(x)\n        x = self.drop_path(x)\n        x = self.norm2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.610943Z","iopub.execute_input":"2024-05-09T10:10:50.611242Z","iopub.status.idle":"2024-05-09T10:10:50.621023Z","shell.execute_reply.started":"2024-05-09T10:10:50.611219Z","shell.execute_reply":"2024-05-09T10:10:50.620262Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class PretrainVisionTransformerEncoder(layers.Layer):\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=LayerNormalization, init_values=None,\n                 use_learnable_pos_emb=False):\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim\n        \n        #         patch embedding\n\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans,\n                                      embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n        \n        # positional encoding       \n\n        if use_learnable_pos_emb:\n            self.pos_embed = tf.Variable(tf.zeros((1, num_patches + 1, embed_dim)))\n        else:\n            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n            \n        #             Transformer blocks\n\n\n        # convert drop_path_rate to a float\n        drop_path_rate = float(drop_path_rate)\n\n        dpr = tf.linspace(0.0, drop_path_rate, depth)  # create a linear space from 0.0 to drop_path_rate\n\n        # convert the TensorFlow tensor to a list of Python floats\n        dpr = dpr.numpy().tolist()\n\n\n        self.blocks = [Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, \n                qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], \n                norm_layer=LayerNormalization,\n                init_values=init_values)\n                       for i in range(depth)]\n        \n        # classification\n#         self.norm = norm_layer(epsilon=1e-6, axis=-1)\n        self.norm = norm_layer(epsilon=1e-6)\n        self.head = Dense(num_classes, input_shape=(embed_dim,)) if num_classes > 0 else tf.identity\n        \n        # Learnable positional embeddings\n        if use_learnable_pos_emb:\n            self.pos_embed = self.add_weight(\n                \"pos_embed\", shape=(1, embed_dim), initializer=trunc_normal_(stddev=0.02)\n            )\n\n    def reset_classifier(self, num_classes):\n        self.num_classes = num_classes\n        self.head = Dense(num_classes, kernel_initializer=RandomNormal(stddev=0.02))\n\n    def call(self, x, mask):\n        x = self.patch_embed(x)\n        x = x + self.pos_embed\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        x = self.head(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.622012Z","iopub.execute_input":"2024-05-09T10:10:50.622298Z","iopub.status.idle":"2024-05-09T10:10:50.636553Z","shell.execute_reply.started":"2024-05-09T10:10:50.622275Z","shell.execute_reply":"2024-05-09T10:10:50.635664Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class PretrainVisionTransformerDecoder(tf.keras.layers.Layer):\n    def __init__(self, patch_size=16, num_classes=768, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., \n                 attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=LayerNormalization, init_values=None,num_patches=196,):\n        super().__init__()\n        self.num_classes = num_classes\n        assert num_classes == 3 * patch_size ** 2\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.patch_size = patch_size\n        \n        drop_path_rate = float(drop_path_rate)\n        # drop probabilities using TensorFlow and numpy\n        dpr = np.linspace(0, drop_path_rate, depth).tolist()\n\n        # convert the list of drop probabilities to TensorFlow tensors\n        dpr = [tf.convert_to_tensor(value=prob, dtype=tf.float32) for prob in dpr]\n        self.blocks = [Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, \n                             qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], \n                norm_layer=LayerNormalization,\n                init_values=init_values)\n                       for i in range(depth)]\n        \n#         self.norm = norm_layer(epsilon=1e-6, axis=-1)\n        self.norm = norm_layer(epsilon=1e-6)\n        self.head = Dense(num_classes, input_shape=(embed_dim,)) if num_classes > 0 else tf.identity\n\n\n    def reset_classifier(self, num_classes):\n        self.num_classes = num_classes\n        self.head = Dense(num_classes, kernel_initializer=GlorotUniform())\n\n    def call(self, x, return_token_num):\n        for blk in self.blocks:\n            x = blk(x)\n\n        if return_token_num > 0:\n            x = self.head(self.norm(x[:, -return_token_num:, :]))  # Return only the mask tokens to predict pixels\n        else:\n            x = self.head(self.norm(x))  # [B, N, 3*16^2]\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.637742Z","iopub.execute_input":"2024-05-09T10:10:50.638074Z","iopub.status.idle":"2024-05-09T10:10:50.650045Z","shell.execute_reply.started":"2024-05-09T10:10:50.638049Z","shell.execute_reply":"2024-05-09T10:10:50.649283Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class PretrainVisionTransformer(tf.keras.Model):\n    def __init__(self,\n                 img_size=224, \n                 patch_size=16, \n                 encoder_in_chans=3, \n                 encoder_num_classes=0, \n                 encoder_embed_dim=768, \n                 encoder_depth=12,\n                 encoder_num_heads=12, \n                 decoder_num_classes=768, \n                 decoder_embed_dim=512, \n                 decoder_depth=8,\n                 decoder_num_heads=8, \n                 mlp_ratio=4., \n                 qkv_bias=False, \n                 qk_scale=None, \n                 drop_rate=0., \n                 attn_drop_rate=0.,\n                 drop_path_rate=0., \n                 norm_layer=LayerNormalization, \n                 init_values=0.,\n                 use_learnable_pos_emb=False,\n                 num_classes=0, # avoid the error from create_fn in timm\n                 in_chans=0, # avoid the error from create_fn in timm\n                 ):\n        super().__init__()\n\n        self.encoder = PretrainVisionTransformerEncoder(\n            img_size=img_size, \n            patch_size=patch_size, \n            in_chans=encoder_in_chans, \n            num_classes=encoder_num_classes, \n            embed_dim=encoder_embed_dim, \n            depth=encoder_depth,\n            num_heads=encoder_num_heads, \n            mlp_ratio=mlp_ratio, \n            qkv_bias=qkv_bias, \n            qk_scale=qk_scale, \n            drop_rate=drop_rate, \n            attn_drop_rate=attn_drop_rate,\n            drop_path_rate=drop_path_rate, \n            norm_layer=norm_layer, \n            init_values=init_values,\n            use_learnable_pos_emb=use_learnable_pos_emb)\n\n        self.decoder = PretrainVisionTransformerDecoder(\n            patch_size=patch_size, \n            num_patches=self.encoder.patch_embed.num_patches,\n            num_classes=decoder_num_classes, \n            embed_dim=decoder_embed_dim, \n            depth=decoder_depth,\n            num_heads=decoder_num_heads, \n            mlp_ratio=mlp_ratio, \n            qkv_bias=qkv_bias, \n            qk_scale=qk_scale, \n            drop_rate=drop_rate, \n            attn_drop_rate=attn_drop_rate,\n            drop_path_rate=drop_path_rate, \n            norm_layer=norm_layer, \n            init_values=init_values)\n\n        self.encoder_to_decoder = Dense(decoder_embed_dim, activation=None, use_bias=False)\n\n        self.mask_token = tf.Variable(tf.zeros((1, 1, decoder_embed_dim)))\n\n        num_patches = self.encoder.patch_embed.num_patches\n        self.pos_embed = get_sinusoid_encoding_table(self.encoder.patch_embed.num_patches, decoder_embed_dim)\n\n        # Initialize the mask token using truncated normal distribution\n        self.mask_token.assign(tf.random.truncated_normal(self.mask_token.shape, stddev=0.02))\n\n    def call(self, x, mask):\n        x_vis = self.encoder(x, mask)  # [B, N_vis, C_e]\n        x_vis = self.encoder_to_decoder(x_vis)  # [B, N_vis, C_d]\n\n        B, N, C = x_vis.shape\n        \n        # Prepare positional embeddings based on mask\n        expand_pos_embed = tf.tile(self.pos_embed, [B, 1, 1])\n        pos_emd_vis = tf.boolean_mask(expand_pos_embed, ~mask, axis=1)  # Visible positional embeddings\n        pos_emd_mask = tf.boolean_mask(expand_pos_embed, mask, axis=1)  # Masked positional embeddings\n        \n        # Concatenate input features with positional embeddings\n        x_full = tf.concat([x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], axis=1)\n        \n        # Decode the masked token predictions\n        x = self.decoder(x_full, pos_emd_mask.shape[1])  # [B, N_mask, 3 * 16 * 16]\n        return x\n\n# Helper function to initialize weights\ndef _init_weights(m):\n    if isinstance(m, Dense):\n        GlorotUniform()(m.weights)\n        if m.bias is not None:\n            Zeros()(m.bias)\n    elif isinstance(m, LayerNormalization):\n        Zeros()(m.bias)\n        tf.ones_like(m.weights)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.651229Z","iopub.execute_input":"2024-05-09T10:10:50.651771Z","iopub.status.idle":"2024-05-09T10:10:50.669175Z","shell.execute_reply.started":"2024-05-09T10:10:50.651739Z","shell.execute_reply":"2024-05-09T10:10:50.668327Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"pre_transformer = PretrainVisionTransformer()","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:50.670239Z","iopub.execute_input":"2024-05-09T10:10:50.670489Z","iopub.status.idle":"2024-05-09T10:10:53.625557Z","shell.execute_reply.started":"2024-05-09T10:10:50.670467Z","shell.execute_reply":"2024-05-09T10:10:53.623831Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pre_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainVisionTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[12], line 28\u001b[0m, in \u001b[0;36mPretrainVisionTransformer.__init__\u001b[0;34m(self, img_size, patch_size, encoder_in_chans, encoder_num_classes, encoder_embed_dim, encoder_depth, encoder_num_heads, decoder_num_classes, decoder_embed_dim, decoder_depth, decoder_num_heads, mlp_ratio, qkv_bias, qk_scale, drop_rate, attn_drop_rate, drop_path_rate, norm_layer, init_values, use_learnable_pos_emb, num_classes, in_chans)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      3\u001b[0m              img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m, \n\u001b[1;32m      4\u001b[0m              patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m              in_chans\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;66;03m# avoid the error from create_fn in timm\u001b[39;00m\n\u001b[1;32m     25\u001b[0m              ):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainVisionTransformerEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_in_chans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_num_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_num_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqkv_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqk_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqk_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_drop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_drop_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_path_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_path_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_learnable_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_learnable_pos_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m PretrainVisionTransformerDecoder(\n\u001b[1;32m     47\u001b[0m         patch_size\u001b[38;5;241m=\u001b[39mpatch_size, \n\u001b[1;32m     48\u001b[0m         num_patches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mpatch_embed\u001b[38;5;241m.\u001b[39mnum_patches,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m         norm_layer\u001b[38;5;241m=\u001b[39mnorm_layer, \n\u001b[1;32m     60\u001b[0m         init_values\u001b[38;5;241m=\u001b[39minit_values)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_to_decoder \u001b[38;5;241m=\u001b[39m Dense(decoder_embed_dim, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","Cell \u001b[0;32mIn[10], line 35\u001b[0m, in \u001b[0;36mPretrainVisionTransformerEncoder.__init__\u001b[0;34m(self, img_size, patch_size, in_chans, num_classes, embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_scale, drop_rate, attn_drop_rate, drop_path_rate, norm_layer, init_values, use_learnable_pos_emb)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# convert the TensorFlow tensor to a list of Python floats\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         dpr \u001b[38;5;241m=\u001b[39m dpr\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m [Block(\n\u001b[1;32m     36\u001b[0m                 dim\u001b[38;5;241m=\u001b[39membed_dim, num_heads\u001b[38;5;241m=\u001b[39mnum_heads, mlp_ratio\u001b[38;5;241m=\u001b[39mmlp_ratio, qkv_bias\u001b[38;5;241m=\u001b[39mqkv_bias, \n\u001b[1;32m     37\u001b[0m                 qk_scale\u001b[38;5;241m=\u001b[39mqk_scale,\n\u001b[1;32m     38\u001b[0m                 drop\u001b[38;5;241m=\u001b[39mdrop_rate, attn_drop\u001b[38;5;241m=\u001b[39mattn_drop_rate, drop_path\u001b[38;5;241m=\u001b[39mdpr[i], \n\u001b[1;32m     39\u001b[0m                 norm_layer\u001b[38;5;241m=\u001b[39mLayerNormalization,\n\u001b[1;32m     40\u001b[0m                 init_values\u001b[38;5;241m=\u001b[39minit_values)\n\u001b[1;32m     41\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)]\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# classification\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#         self.norm = norm_layer(epsilon=1e-6, axis=-1)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m norm_layer(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n","Cell \u001b[0;32mIn[10], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# convert the TensorFlow tensor to a list of Python floats\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         dpr \u001b[38;5;241m=\u001b[39m dpr\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m [\u001b[43mBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqkv_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                \u001b[49m\u001b[43mqk_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqk_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_drop_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLayerNormalization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                \u001b[49m\u001b[43minit_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)]\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# classification\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#         self.norm = norm_layer(epsilon=1e-6, axis=-1)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m norm_layer(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: Block.__init__() missing 2 required positional arguments: 'act_layer' and 'attn_head_dim'"],"ename":"TypeError","evalue":"Block.__init__() missing 2 required positional arguments: 'act_layer' and 'attn_head_dim'","output_type":"error"}]},{"cell_type":"code","source":"def pretrain_mae_base_patch16_224(pretrained=False, **kwargs):\n    # Define the model architecture parameters\n    model = PretrainVisionTransformer(\n        img_size=224,\n        patch_size=16, \n        encoder_embed_dim=768, \n        encoder_depth=12, \n        encoder_num_heads=12,\n        encoder_num_classes=0,\n        decoder_num_classes=768,\n        decoder_embed_dim=384,\n        decoder_depth=4,\n        decoder_num_heads=6,\n        mlp_ratio=4, \n        qkv_bias=True,\n        norm_layer=partial(LayerNormalization(epsilon=1e-6)),\n        **kwargs)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:53.626507Z","iopub.status.idle":"2024-05-09T10:10:53.626864Z","shell.execute_reply.started":"2024-05-09T10:10:53.626679Z","shell.execute_reply":"2024-05-09T10:10:53.626692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = pretrain_mae_base_patch16_224()","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:53.628052Z","iopub.status.idle":"2024-05-09T10:10:53.628400Z","shell.execute_reply.started":"2024-05-09T10:10:53.628230Z","shell.execute_reply":"2024-05-09T10:10:53.628245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:53.629641Z","iopub.status.idle":"2024-05-09T10:10:53.630045Z","shell.execute_reply.started":"2024-05-09T10:10:53.629800Z","shell.execute_reply":"2024-05-09T10:10:53.629815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{}},{"cell_type":"code","source":"class RandomMaskingGenerator:\n    def __init__(self, input_size, mask_ratio):\n        self.input_size = input_size\n        self.mask_ratio = mask_ratio\n\n    def __call__(self):\n        num_patches = self.input_size[0] * self.input_size[1]\n        num_mask = int(self.mask_ratio * num_patches)\n        mask = np.hstack([np.zeros(num_patches - num_mask), np.ones(num_mask)])\n        np.random.shuffle(mask)\n        return mask.astype(int)\n\ndef build_pretraining_dataset(args):\n\n    image_data_generator = ImageDataGenerator(\n        horizontal_flip=True,\n        vertical_flip=False,\n        rescale=1/255,\n    )\n\n    def custom_generator(image_paths):\n        for img_path in image_paths:\n            img = Image.open(img_path)  \n            img_array = np.array(img)\n            augmented_img = image_data_generator.random_transform(img_array)\n            masked_positions = RandomMaskingGenerator(augmented_img.shape[:2], args.mask_ratio)()\n            yield augmented_img, masked_positions\n\n    return custom_generator(args.data_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T10:10:53.631367Z","iopub.status.idle":"2024-05-09T10:10:53.631693Z","shell.execute_reply.started":"2024-05-09T10:10:53.631533Z","shell.execute_reply":"2024-05-09T10:10:53.631546Z"},"trusted":true},"execution_count":null,"outputs":[]}]}