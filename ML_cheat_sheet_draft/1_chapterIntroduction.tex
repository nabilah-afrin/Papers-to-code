\documentclass[graybox, envcountchap, twocolumn]{styles/svmult}
\usepackage{fontspec}  % For custom fonts
\usepackage{polyglossia}  % For language support

% Set English and Bangla as languages
\setdefaultlanguage{english}
\setotherlanguage{bengali}

% Set fonts for English and Bangla
\newfontfamily\bengalifont[Script=Bengali]{Kalpurush}  % You can change 'Kalpurush' to another Bangla font like 'SolaimanLipi' or 'Noto Sans Bengali'

\usepackage{amssymb,amsmath,bm}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\usepackage{textcomp}
\newcommand\abs[1]{\left\lvert#1\right\rvert}
\usepackage{longtable}
\usepackage{algorithm2e}
\usepackage{tocbibind}
\usepackage[toc]{multitoc}
\renewcommand{\bibname}{References}
\usepackage{mathptmx}  % Times Roman as basic font
\usepackage{helvet}    % Helvetica as sans-serif font
\usepackage{courier}   % Courier as typewriter font

\usepackage{makeidx}   % Index generation
\usepackage{graphicx}  % Standard LaTeX graphics tool
\usepackage[justification=centering]{caption}
\usepackage{subfig}
\usepackage{multicol}  % For multi-column index
\usepackage{multirow}
\usepackage[bottom]{footmisc}  % Footnotes at the bottom
\usepackage[bookmarksnumbered=true,
            bookmarksopen=true,
            colorlinks=true,
            linkcolor=blue,
            anchorcolor=blue,
            citecolor=blue]{hyperref}

\graphicspath{{figures/}}

\makeindex  % For subject index generation

\begin{document}

\section{Introduction}

\subsection{Types of Machine Learning}
% Here are the types of machine learning and their explanations in Bangla:

\begin{itemize}
    \item \textbf{Supervised Learning}
    \begin{itemize}
        \item \textbf{Classification}: {\bengalifont ржбрзЗржЯрж╛ржХрзЗ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐рждрзЗ ржнрж╛ржЧ ржХрж░рж╛}
        \item \textbf{Regression}: {\bengalifont ржПржХржЯрж┐ ржХржирзНржЯрж┐ржирж┐ржЙржпрж╝рж╛рж╕ рж░рзЗржЬрж╛рж▓рзНржЯржХрзЗ ржкрзНрж░рзЗржбрж┐ржХрзНржЯ ржХрж░рж╛}
    \end{itemize}
    \item \textbf{Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Clusters}: {\bengalifont ржПржХ рж░ржХржорзЗрж░ ржбрзЗржЯрж╛ ржкржпрж╝рзЗржирзНржЯржЧрзБрж▓рзЛржХрзЗ ржПржХрж╕рж╛ржерзЗ ржЧрзНрж░рзБржк ржХрж░рж╛}
        \item \textbf{Discovering latent factors}: {\bengalifont ржбрзЗржЯрж╛рж░ ржоржзрзНржпрзЗ рж▓рзБржХрж╛ржирзЛ ржлрзНржпрж╛ржХрзНржЯрж░ ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж╛}
        \item \textbf{Discovering graph structure}: {\bengalifont ржбрзЗржЯрж╛рж░ ржоржзрзНржпрзЗ ржмрж┐ржнрж┐ржирзНржи рж╕ржорзНржкрж░рзНржХ ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж╛, ржпрзЗржЦрж╛ржирзЗ ржбрзЗржЯрж╛ржХрзЗ ржирзЛржб ржПржмржВ ржПржЬ ржжрж┐ржпрж╝рзЗ ржЧрзНрж░рж╛ржл ржЖржХрж╛рж░рзЗ ржжрзЗржЦрж╛ржирзЛ ржпрж╛ржпрж╝}
        \item \textbf{Matrix completion}: {\bengalifont ржХрзЛржерж╛ржУ ржбрзЗржЯрж╛ ржорзНржпрж╛ржЯрзНрж░рж┐ржХрзНрж╕рзЗрж░ ржХрж┐ржЫрзБ ржЕржВрж╢ ржорж┐рж╕рж┐ржВ ржерж╛ржХрж▓рзЗ, рж╕рзЗржЯрж╛ ржкрзВрж░ржг ржХрж░рж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рж╛ рж╣ржпрж╝}
    \end{itemize}
\end{itemize}


\section{{\bengalifont ржорзЗрж╢рж┐ржи рж▓рж╛рж░рзНржирж┐ржВ ржоржбрзЗрж▓рзЗрж░ рждрж┐ржиржЯрж┐ ржзрж╛ржк}}

\textbf{Model = Representation + Evaluation + Optimization}\footnote{Domingos, P. A few useful things to know about machine learning. Commun. ACM. 55(10):78тАУ87 (2012).}


\subsection{Representation}

\bengalifont
Supervised Learning- ржПрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржоржбрзЗрж▓ржХрзЗ рж╕ржмрж╕ржорзЯ рждрзИрж░рзА ржХрж░рждрзЗ рж╣ржмрзЗ conditional probability distribution $P(y|\vec{x})$ ржЖржХрж╛рж░рзЗ ржЕржержмрж╛ decision function $f(x)$ рж╣рж┐рж╕рзЗржмрзЗред  ржПржЗ рж░рж┐ржкрзНрж░рзЗрж╕рзЗржирзНржЯрзЗрж╢ржи
ржХрзНрж▓рж╛рж╕рж┐ржлрж┐ржХрзЗрж╢ржирзЗрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржпржжрж┐ ржзрж░рж┐ , ржПржЗ ржХржирзНржбрж┐рж╢ржирж╛рж▓ ржбрж┐рж╕рзНржЯрзНрж░рж┐ржмрж┐ржЙрж╢ржирзЗрж░ ржорж╛ржзрзНржпржорзЗ ржЖржорж░рж╛ ржмрзЗрж░ ржХрж░рждрзЗ ржкрж╛рж░ржЫрж┐ ржХрзЛржирзЛ ржЗржиржкрзБржЯ  $f(x)$ ржжрзЗржпрж╝рж╛рж░ ржкрж░ ржХрзЛржи ржХрзНрж▓рж╛рж╕ рж▓рзЗржмрзЗрж▓ y ржкрж╛ржУржпрж╝рж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛ ржХрждржЯрзБржХрзБ ржЖржЫрзЗ. ржорзЗрж╢рж┐ржи рж▓рж╛рж░рзНржирж┐ржВрзЯрзЗрж░ ржнрж╛рж╖рж╛рзЯ ржПржЗ ржбрж┐рж╕рзНржЯрзНрж░рж┐ржмрж┐ржЙрж╢ржиржХрзЗ ржХрзНрж▓рж╛рж╕рж┐ржлрж╛рзЯрж╛рж░ ржмрж▓рж╛ рж╣рзЯ. ржПржЗ рж╕ржХрж▓ ржХрзНрж▓рж╛рж╕рж┐ржлрж╛рзЯрж╛рж░ржХрзЗ ржирж┐рзЯрзЗ ржПржХрж╕рж╛ржерзЗ ржпрзЗржЗ set рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯ рждрж╛ржХрзЗ hypothesis space ржмрж▓рзЗред 



% 1.2.2 
\subsection{Evaluation}
{\bengalifont
hypothesis space ржПрж░ ржоржзрзНржпрзЗ ржерж╛ржХрж╛, ржХрзЛржиржЯрж╛ ржнрж╛рж▓ classifier ржПржмржВ ржХрзЛржиржЯрж╛ ржЦрж╛рж░рж╛ржк classifer рж╕рзЗржЯрж╛ ржмрзБржЭрж╛рж░ ржЬржирзНржпрзЗ evaluation function (objective function or risk function) ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯред ржоржбрзЗрж▓ ржпржЦржи classifier-ржжрзЗрж░ ржоржзрзНржпрзЗ ржкрж╛рж░рзНржержХрзНржп ржХрж░рждрзЗ ржЪрж╛рзЯ, рждржЦржи ржПржЗ evaluation function ржПржХржЯрж╛ рж╕рзНржХрзЛрж░ ржмрж╛ ржнрзНржпрж╛рж▓рзБ рж░рж┐ржЯрж╛рж░рзНржи ржХрж░рзЗ ржпрзЗржЯрж╛рж░ ржорж╛ржзрзНржпржорзЗ learning algorithm ржзрж░рждрзЗ ржкрж╛рж░рзЗ ржХрзЛржи classifier рж╕ржмржЪрзЗрзЯрзЗ ржнрж╛рж▓ред}


% 1.2.2. 1 
\subsubsection{Loss function \& risk function}
\label{sec:Loss-function-and-risk-function}

\begin{definition}
\textbf{Loss Function}

{\bengalifont рж╣рж╛ржЗржкрзЛржерзЗрж╕рж┐рж╕ рж╕рзНржкрзЗрж╕ (hypothesis space) ржбрж┐ржлрж╛ржЗржи ржХрж░рж╛рж░ ржкрж░рзЗ ржПржнрж╛рж▓рзБрзЯрзЗрж╢ржи (evaluation) ржкрзНрж░рж╕рзЗрж╕ ржПрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржкрзНрж░ржержо ржзрж╛ржк рж╣ржЪрзНржЫрзЗ ржкрзНрж░рждрзНржпрзЗржХрзНржЯрж╛ ржХрзНрж▓рж╛рж╕рж┐ржлрж╛рзЯрж╛рж░рзЗ рж▓рж╕ ржлрж╛ржВрж╢ржи ржкрзНрж░рзЯрзЛржЧ ржХрж░рж╛, ржпрзЗржЯрж╛ ржмрзБржЭрж╛ржмрзЗ ржпрзЗ ржПржХржЯрж╛ classifer ржПрж░ ржкрзНрж░рзЗржбрж┐ржХрж╢ржи ржХрждржЯрзБржХрзБ ржЯрзНрж░рзЗржирж┐ржВ рж╕рзЗржЯ ржПрж░ рж╕рж╛ржерзЗ ржорзНржпрж╛ржЪ ржХрж░рждрзЗ ржкрзЗрж░рзЗржЫрзЗред ржПржХрзНрж╖рзЗрждрзНрж░рзЗ ржкрзНрж░рждрж┐ржЯрж╛ ржкрзНрж░рзЗржбрж┐ржХрж╢ржирзЗрж░ ржЙржкрж░ рж▓рж╕ ржлрж╛ржВрж╢ржи ржкрзНрж░рзЯрзЛржЧ ржХрж░рж╛ рж╣рзЯ, рж▓рж╛рж░рзНржирж┐ржВ ржПрж▓ржЧрзЛрж░рж┐ржжржо ржЯрзНрж░рзЗржирж┐ржВ ржбрж╛ржЯрж╛рж░ ржЙржкрж░ ржЧрзЬ (mean) ржмрж╛ рж╕ржорзНржкрзВрж░рзНржг (total) рж▓рж╕ ржХржорж┐рзЯрзЗ рж╕ржмржЪрзЗрзЯрзЗ ржнрж╛рж▓рзЛ ржкрж╛рж░ржлрж░рзНржо ржХрж░рж╛ classifer-ржХрзЗ ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рзЗред}

\newline


{\bengalifont ржПржХржЯрж╛ ржлрж╛ржВрж╢ржи ржХржд ржнрж╛рж▓ржнрж╛ржмрзЗ ржЯрзНрж░рзЗржЗржирж┐ржВ ржбрж╛ржЯрж╛ ржПрж░ ржЙржкрж░ ржлрж┐ржЯ рж╕рзЗржЯрж╛ ржкрж░рж┐ржорж╛ржк ржХрж░рж╛рж░ ржЬржирзНржп ржПржХржЯрж╛ рж▓рж╕ ржлрж╛ржВрж╢ржи (loss function) рж╕ржВржЬрзНржЮрж╛рзЯрж┐ржд ржХрж░рж┐ред ржПржХржЯрж┐ ржЯрзНрж░рзЗржЗржирж┐ржВ ржПржХрзНрж╕рж╛ржорзНржкрж▓ $(x_i, y_i)$  ржПрж░ ржЬржирзНржп ржнрзНржпрж╛рж▓рзБ $y_hat$ ржХрзЗ ржкрзНрж░рзЗржбрж┐ржХрзНржЯ ржХрж░рждрзЗ рж▓рж╕ рж╣ржмрзЗ $L(y, y_hat)$}


\begin{equation}
    \textbf{loss function} $L:Y \times Y \rightarrow R \geq 0$  
    % L:Y├ЧYтЖТRтЙе0
\end{equation}

\begin{itemize}
    \item $ Y \times Y $ : {\bengalifont ржПржЦрж╛ржирзЗ ржмрзЛржЭрж╛ржирзЛ рж╣ржЪрзНржЫрзЗ ржпрзЗ рж▓рж╕ ржлрж╛ржВрж╢ржи рж╕ржХрж▓ рж╕ржорзНржнрж╛ржмрзНржп label ржмрж╛ output ржПрж░ рж╕рзЗржЯ ржерзЗржХрзЗ ржжрзБржЗржЯрж╛ ржЖрж░рзНржЧрзБржорзЗржирзНржЯ ржирзЗржпрж╝;}
    \begin{itemize}
        \item $ y_i $: ith {\bengalifont ржЯрзНрж░рзЗржирж┐ржВ ржПржХрзНрж╕рж╛ржорзНржкрж▓рзЗрж░ ржЖрж╕рж▓ (actual) рж▓рзЗржмрзЗрж▓ред}
        \item $\widehat{y}$ : {\bengalifont ржоржбрзЗрж▓ ржпрзЗ ржкрзНрж░рзЗржбрж┐ржХрж╢ржи ржжрж┐рзЯрзЗржЫрзЗред }
    \end{itemize}
    \item $R \geq 0$ : {\bengalifont рж▓рж╕ ржлрж╛ржВрж╢ржирзЗрж░ ржЖржЙржЯржкрзБржЯ рж╣рж▓рзЛ ржПржХржЯрж╛ ржиржи-ржирзЗржЧрзЗржЯрж┐ржн рж░рж┐ржпрж╝рзЗрж▓ рж╕ржВржЦрзНржпрж╛ (ржпрзЗржЯрж╛ $R \geq 0$ ржжрж┐ржпрж╝рзЗ ржкрзНрж░ржХрж╛рж╢ ржХрж░рж╛ рж╣ржпрж╝)ред ржПржЗ рж╕ржВржЦрзНржпрж╛ржЯрж╛ ржжрзЗржЦрж╛ржпрж╝ ржХрждржЯрж╛ ржПрж░рж░ ржмрж╛ "рж▓рж╕" ржЖржЫрзЗ actual ржмрж╛ ржЖрж╕рж▓ рж▓рзЗржмрзЗрж▓ $ y_i $  ржЖрж░ ржкрзНрж░рзЗржбрж┐ржХрзНржЯ ржХрж░рж╛ рж▓рзЗржмрзЗрж▓ $\widehat{y}$ -ржПрж░ ржоржзрзНржпрзЗред ржЖржорж╛ржжрзЗрж░ рж▓ржХрзНрж╖рзНржп рж╣рж▓рзЛ ржПржЗ ржорж╛ржиржЯрж╛ ржпрждржЯрж╛ рж╕ржорзНржнржм ржХржорж┐ржпрж╝рзЗ ржЖржирж╛ред}
\end{itemize}


\end{definition}
{\bengalifont
% The following is some common loss functions:
ржирж┐ржЪрзЗ ржХрж┐ржЫрзБ рж╕рж╛ржзрж╛рж░ржг рж▓рж╕ ржлрж╛ржВрж╢ржирзЗрж░ ржЙржжрж╛рж╣рж░ржг ржжрзЗржУржпрж╝рж╛ рж╣рж▓рзЛ:}
\begin{itemize}


\item 0-1 loss function \\ $L(Y,f(X))=\mathbb{I}(Y,f(X))=\begin{cases} 1, & Y=f(X) \\ 0, & Y \neq f(X) \end{cases}$ %L(Y,f(X))=I(Y,f(X))

\begin{itemize}
    \item \mathbb{I}(Y,f(X)) : {\bengalifont ржПржХржЯрж╛ ржЗржирзНржбрж┐ржХрзЗржЯрж░ ржлрж╛ржВрж╢ржи, ржпрзЗржЦрж╛ржирзЗ ржпржжрж┐ actual рж▓рзЗржмрзЗрж▓  $ y_i $  ржЖрж░ ржкрзНрж░рзЗржбрж┐ржХрзНржЯрзЗржб рж▓рзЗржмрзЗрж▓  $f(X)$  ржирж╛ ржорж┐рж▓рзЗ, рждрж╛рж╣рж▓рзЗ ржЖржЙржЯржкрзБржЯ рж╣ржмрзЗ 1, ржЖрж░ ржорж┐рж▓рж▓рзЗ ржЖржЙржЯржкрзБржЯ рж╣ржмрзЗ 0ред}
    \item {\bengalifont ржПржЯрж╛ ржЦрзБржмржЗ рж╕рж╛ржзрж╛рж░ржг ржПржХржЯрж╛ рж▓рж╕ ржлрж╛ржВрж╢ржи, ржпрзЗржЯрж╛ рж╢рзБржзрзБ ржЪрзЗржХ ржХрж░рзЗ ржкрзНрж░рзЗржбрж┐ржХрж╢ржи ржарж┐ржХ ржЖржЫрзЗ ржирж╛ржХрж┐ ржнрзБрж▓, ржнрзБрж▓рзЗрж░ ржкрж░рж┐ржорж╛ржг ржЧрзБрж░рзБрждрзНржм ржжрзЗржпрж╝ ржирж╛ред}
\end{itemize}


\item Quadratic loss function $L(Y,f(X))=\left(Y-f(X)\right)^2$ %L(Y,f(X))=(YтИТf(X))2
\begin{itemize}
    \item $YтИТf(X)$ {\bengalifont рж╣рж▓рзЛ ржЖрж╕рж▓ рж▓рзЗржмрзЗрж▓ $Y$ ржЖрж░ ржкрзНрж░рзЗржбрж┐ржХрзНржЯрзЗржб рж▓рзЗржмрзЗрж▓ $f(X)$ ржПрж░ ржоржзрзНржпрзЗ ржкрж╛рж░рзНржержХрзНржп рж╕рзНржХрзЯрж╛рж░ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ ржирзЗржЧрзЗржЯрж┐ржн ржнрзНржпрж╛рж▓рзБржХрзЗ ржкрж░рж┐рж╣рж╛рж░ ржХрж░рж╛рж░ ржЬржирзНржпрзЗред }
    \item Mean Squared Error {\bengalifont ржирж╛ржорзЗржЗ ржЪрзЗржирж╛ ржПржЗ рж▓рж╕ ржлрж╛ржВрж╢ржи regression ржкрзНрж░ржмрж▓рзЗржорзЗрж░ ржЬржирзНржп ржЕржирзЗржХ ржмрзЗрж╢рж┐ ржмрзНржпржмрж╣рзГржд рж╣ржпрж╝ ржпрзЗржЦрж╛ржирзЗ ржЖрж╕рж▓ ржЖрж░ ржкрзНрж░рзЗржбрж┐ржХрзНржЯрзЗржб ржнрзНржпрж╛рж▓рзБрж░ ржоржзрзНржпрзЗ ржпржд ржмрзЗрж╢рж┐ ржкрж╛рж░рзНржержХрзНржп, рждржд ржмрзЗрж╢рж┐ рж▓рж╕ред }
\end{itemize}

\item Absolute loss function $L(Y,f(X))=\abs{Y-f(X)}$  % L(Y,f(X))=|YтИТf(X)|
\begin{itemize}
    \item $\abs{Y-f(X)}$  {\bengalifont рж╣рж▓рзЛ ржЖрж╕рж▓ рж▓рзЗржмрзЗрж▓ $y$ ржПржмржВ  ржЖрж░ ржкрзНрж░рзЗржбрж┐ржХрзНржЯрзЗржб рж▓рзЗржмрзЗрж▓ $f(X)$ ржПрж░ ржоржзрзНржпрзЗ ржЕрзНржпрж╛ржмрж╕рзЛрж▓рж┐ржЙржЯ ржкрж╛рж░рзНржержХрзНржпред} 
    \item {\bengalifont ржПржЗ ржлрж╛ржВрж╢ржи ржЖрж╕рж▓ ржЖрж░ ржкрзНрж░рзЗржбрж┐ржХрзНржЯрзЗржб ржнрзНржпрж╛рж▓рзБрж░ ржоржзрзНржпрзЗ рж╕рж░рж╛рж╕рж░рж┐ ржкрж╛рж░рзНржержХрзНржп ржжрзЗржпрж╝, рж╕рзНржХрзЛржпрж╝рж╛рж░ ржирж╛ ржХрж░рзЗред ржпрзЗрж╕ржХрж▓ ржХрзНрж╖рзЗрждрзНрж░рзЗ average loss ржмрж╛ error ржжрзЗржЦрж╛рж░ ржжрж░ржХрж╛рж░ ржкрзЬрзЗ рж╕рзЗржЦрж╛ржирзЗ ржЖржорж░рж╛ ржПржЗ рж▓рж╕ ржлрж╛ржВрж╢ржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржерж╛ржХрж┐ред }
\end{itemize}
\item Logarithmic loss function \\ $L(Y,P(Y|X))=-\log{P(Y|X)}$ % L(Y,P(YтИгX))=тИТlogP(YтИгX)
\begin{itemize}
    \item $P(Y|X)$  {\bengalifont рж╣рж▓рзЛ $X$ ржЗржиржкрзБржЯ ржжрзЗржУржпрж╝рж╛рж░ ржкрж░ ржЖрж╕рж▓ рж▓рзЗржмрзЗрж▓ $y$ ржкрж╛ржУрзЯрж╛рж░ ржкрзНрж░рзЗржбрж┐ржХрзНржЯрзЗржб ржкрзНрж░рзЛржмрж╛ржмрж┐рж▓рж┐ржЯрж┐ред  $-\log{P(Y|X)}$ ржкрзНрж░рзЗржбрж┐ржХрзНржЯрзЗржб ржкрзНрж░рзЛржмрж╛ржмрж┐рж▓рж┐ржЯрж┐рж░ рж▓рзЛржЧрж╛рж░рж┐ржжржо ржирж┐ржпрж╝рзЗ рждрж╛рж░ ржирзЗржЧрзЗржЯрж┐ржн ржирзЗржУржпрж╝рж╛ рж╣ржпрж╝, ржХрж╛рж░ржг ржЖржорж░рж╛ ржЪрж╛ржЗ high ржкрзНрж░рзЛржмрж╛ржмрж┐рж▓рж┐ржЯрж┐ ржПрж░ ржЬржирзНржпрзЗ ржпрзЗржи ржХржо loss value ржЖрж╕рзЗред }
    \item { ржпржЦржи $P(Y|X)$ ржПрж░ probability score low, Logarithmic loss function рждржЦржи ржнрзБрж▓ prediction ржХрзЗ penalize ржХрж░рзЗ; ржПржЗ ржХрж╛рж░ржгрзЗ ржПржЗ loss function classifcation ржкрзНрж░ржмрж▓рзЗржорзЗ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯред }
\end{itemize}

\end{itemize}

\begin{definition}
$R_{\mathrm{exp}}(f)$ {\bengalifont рж╣ржЪрзНржЫрзЗ expected loss ржмрж╛ \textbf{risk function}; ржпрж╛ ржжрзНржмрж╛рж░рж╛ ржмрзЛржЭрж╛ржпрж╝ ржпрзЗ ржХрзЛржирзЛ ржлрж╛ржВрж╢ржи $f$ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рзЗржбрж┐ржХрж╢ржи ржХрж░рж╛рж░ рж╕ржоржпрж╝ ржХрждржЯрж╛ error (loss) ржерж╛ржХрждрзЗ ржкрж╛рж░рзЗ}
% The risk of function $f$ is defined as the expected loss of $f$:
\begin{equation}\label{eqn:expected-loss} % Rexp(f)=E[L(Y,f(X))]= тИл L(y,f(x))P(x,y)dxdy
R_{\mathrm{exp}}(f)=E\left[L\left(Y,f(X)\right)\right]=\int L\left(y,f(x)\right)P(x,y)\mathrm{d}x\mathrm{d}y
\end{equation}
\begin{itemize}
    \item $L\left(Y,f(X)\right)\right$ : loss function
    \item $E[тЛЕ]$ :{\bengalifont  Expectation ржЕржкрж╛рж░рзЗржЯрж░, probability distribution ржПрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗ ржлрж╛ржВрж╢ржирзЗрж░ average expected value ржирж┐рж░рзНржгрзЯ ржХрж░рзЗ }
    \item $P(x,y)$ :{\bengalifont input data $X$ ржПржмржВ рждрж╛рж░ label $Y$ joint probaility distribution, ржпрзЗржЯрж╛ ржЗржиржкрзБржЯ-ржЖржЙржЯржкрзБржЯ ржПрж░ рж╕ржнрж╛ржмрзНржпрждрж╛ ржмрзЗрж░ ржХрж░рзЗ }
    \item {\bengalifont ржПржХрзНрж╕ржкрзЗржХрзНржЯрзЗржб average value ржирж┐рж░рзНржгрзЯ ржХрж░рж╛рж░ ржЬржирзНржпрзЗ integral $\int mathrm{d}x\mathrm{d}y$ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ; рж╕ржорзНржнрж╛ржмрзНржп рж╕ржХрж▓ data point  $X$ ржПржмржВ $Y$ ржПрж░ loss value ржПрж░ average ржмрзЗрж░ ржХрж░ржЫрзЗ probability distribution ржПрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗ ред }
\end{itemize}
% which is also called expected loss or \textbf{risk function}.
\end{definition}


\begin{definition}
training data ржерзЗржХрзЗ The risk function $R_{\mathrm{exp}}(f)$ ржХрзЗ ржЕржирзБржорж╛ржи ржХрж░рж╛рж░ ржлрж╛ржВрж╢ржи-
% The risk function $R_{\mathrm{exp}}(f)$ can be estimated from the training data as
\begin{equation}
R_{\mathrm{emp}}(f)=\dfrac{1}{N}\sum\limits_{i=1}^{N} L\left(y_i,f(x_i)\right) % 
\end{equation}
\begin{itemize}
    \item $R_{\mathrm{emp}}(f)$ {\bengalifont рж╣ржЪрзНржЫрзЗ ржПржоржкрж┐рж░рж┐ржХрж╛рж▓ рж░рж┐рж╕рзНржХ ржмрж╛ ржПржоржкрж┐рж░рж┐ржХрж╛рж▓ рж▓рж╕, ржпрзЗржЯрж╛рж░ ржорж╛ржзрзНржпржорзЗ ржЬрж╛ржирж╛ ржпрж╛рзЯ ржПржХржЯрж╛ ржоржбрзЗрж▓ $f$ рж╢рзБржзрзБ ржЯрзНрж░рзЗржирж┐ржВ ржбрзЗржЯрж╛ржпрж╝ ржХрждрзЛржЯрж╛ ржнрж╛рж▓рзЛ ржХрж╛ржЬ ржХрж░рзЗржЫрзЗред}
    \item {\bengalifont ржЧрзЬ ржорж╛ржи ржкрж╛ржмрж╛рж░ ржЬржирзНржпрзЗ total loss ржПрж░ average ржмрзЗрж░ ржХрж░ржЫрж┐} 
    \item $L\left(y_i,f(x_i)\right)$ {\bengalifont data ржкрзЯрзЗржирзНржЯ $y_i,x_i$ ржПрж░ ржЙржкрж░ loss function apply ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ, $x_i$ ржпрзЗржЦрж╛ржирзЗ ржЗржиржкрзБржЯ ржПржмржВ $y_i$ рж╣рж▓рзЛ ржЖрж╕рж▓ ржЖржЙржЯржкрзБржЯ, ржЖрж░ $f(x_i)$ рж╣рж▓рзЛ ржкрзНрж░рзЗржбрж┐ржХрзНржЯрзЗржб ржЖржЙржЯржкрзБржЯред}
\end{itemize}
\bengalifont
ржПржЗ ржлрж╛ржВрж╢ржиржХрзЗ empirical loss ржмрж╛  \textbf{empirical risk}-ржУ ржмрж▓рж╛ рж╣рзЯрзЗ ржерж╛ржХрзЗ.
% which is also called empirical loss or \textbf{empirical risk}.
\end{definition}

\bengalifont{ржЖржорж░рж╛ ржХрж┐ржирзНрждрзБ ржЪрж╛ржЗрж▓рзЗ ржЖржорж╛ржжрзЗрж░ ржирж┐ржЬрзЗржжрзЗрж░ ржорждрзЛ ржХрж░рзЗржУ рж▓рж╕ ржлрж╛ржВрж╢ржи ржбрж┐ржлрж╛ржЗржи ржХрж░рждрзЗ ржкрж╛рж░рж┐; ржХрж┐ржирзНрждрзБ рж╢рзБрж░рзБрж░ ржжрж┐ржХрзЗ рж╢рзЗржЦрж╛рж░ ржЕржмрж╕рзНржерж╛рзЯ рж▓рж┐ржЯрж╛рж░рзЗржЪрж░ ржерзЗржХрзЗ ржерзЗржХрзЗ ржПржХржЯрж┐ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ ржЖржорж╛ржжрзЗрж░ ржЬржирзНржп ржнрж╛рж▓рзЛ рж╣ржмрзЗред рж▓рж╕ ржлрж╛ржВрж╢ржи ржбрж┐ржлрж╛ржЗржи ржХрж░рж╛рж░ рж╕ржорзЯ ржЕржмрж╢рзНржпржЗ ржХрж┐ржЫрзБ ржмрж┐рж╖рзЯ ржорж╛ржерж╛рзЯ рж░рж╛ржЦрждрзЗ рж╣ржмрзЗ- } \footnote{\url{http://t.cn/zTrDxLO}}


\begin{enumerate}
\item \bengalifont{  ржоржбрзЗрж▓ ржпрзЗржЗ ржЖрж╕рж▓ рж▓рж╕(actual loss) ржХржорж╛ржирзЛрж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░ржЫрзЗ, рж╕рзЗржЗ рж▓рж╕ржХрзЗ ржХрж╛ржЫрж╛ржХрж╛ржЫрж┐ ржЖржирж╛ржЗ рж▓рж╕ ржлрж╛ржВрж╢ржирзЗрж░ ржХрж╛ржЬред ржЙржжрж╛рж╣рж░ржгрж╕рж░рзВржк, ржХрзНрж▓рж╛рж╕рж┐ржлрж┐ржХрзЗрж╢ржирзЗрж░ ржЬржирзНржп ржПржХржЯрж┐ рж╕рж╛ржзрж╛рж░ржг рж▓рж╕ ржлрж╛ржВрж╢ржи рж╣рж▓ ржЬрж┐рж░рзЛ-ржУрзЯрж╛ржи рж▓рж╕, ржпрзЗржЯрж╛ рж╢рзБржзрзБ ржХрждржЧрзБрж▓рзЛ ржнрзБрж▓ ржХрзНрж▓рж╛рж╕рж┐ржлрж┐ржХрзЗрж╢ржи рж╣рзЯрзЗржЫрзЗ рж╕рзЗржЗ рж╣рж┐рж╕рж╛ржм рж░рж╛ржЦрзЗ; ржПржХржЯрж┐ ржнрзБрж▓ ржкрзНрж░рзЗржбрж┐ржХрж╢ржирзЗрж░ ржЬржирзНржп рзз ржПржмржВ рж╕ржарж┐ржХ ржкрзНрж░рзЗржбрж┐ржХрж╢ржирзЗрж░ ржЬржирзНржп рзж ржжрзЗрзЯ}


\item {\bengalifont ржЖржорж░рж╛ ржпрзЗржЗ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЕржкржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рждрзЗ ржЪрж╛ржЗ рждрж╛ржХрзЗ ржЕржмрж╢рзНржпржЗ ржорж╛ржирж╛ржирж╕ржЗ рж╣рждрзЗ рж╣ржмрзЗ рж▓рж╕ ржлрж╛ржВрж╢ржиржХрзЗ ржЕржмрж╢рзНржпржЗ ржЬржирзНржпржЗ ржЬрж┐рж░рзЛ-ржУрзЯрж╛ржи рж▓рж╕ рж╕рж░рж╛рж╕рж░рж┐ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ ржирж╛, ржХрж╛рж░ржг ржПржЯрж╛ ржЧрзНрж░рзЗржбрж┐рзЯрзЗржирзНржЯ-ржнрж┐рждрзНрждрж┐ржХ ржЕржкржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи ржорзЗржержб ржПрж░ рж╕рж╛ржерзЗ ржХрж╛ржЬ ржХрж░рзЗ ржирж╛}


% The main algorithm that optimizes the zero-one-loss directly is the old perceptron algorithm(chapter \S \ref{chap:Perceptron}).
% \end{enumerate}





\subsubsection{ERM & SRM}
\bengalifont
ERM(ERM (Empirical Risk Minimization)) ржПрж░ рж▓ржХрзНрж╖рзНржп рж╣рж▓ ржкрзНрж░рждрзНржпрзЗржХржЯрж╛ ржЯрзНрж░рзЗржЗржирж┐ржВ ржбрж╛ржЯрж╛ ржерзЗржХрзЗ ржкрзНрж░рж╛ржкрзНржд рж▓рж╕ ржлрж╛ржВрж╢ржирзЗрж░ ржПржнрж╛рж░рзЗржЬ ржнрзНржпрж╛рж▓рзБ ржмрзЗрж░ ржХрж░рж╛ ред ржПржЗ ржкржжрзНржзрждрж┐рждрзЗ ржЖржорж░рж╛ рж╣рж╛ржЗржкржерж┐рж╕рж┐рж╕ рж╕рзНржкрзЗржЗрж╕ $ЁЭСУ$ ржерзЗржХрзЗ ржПржоржи ржПржХржЯрж┐ ржлрж╛ржВрж╢ржи $f$ (ржоржбрзЗрж▓ ржмрж╛ ржХрзНрж▓рж╛рж╕рж┐ржлрж╛рзЯрж╛рж░) ржЦрзБржБржЬрзЗ ржкрж╛ржЗ ржпрж╛ ржЯрзНрж░рзЗржирж┐ржВ ржбрзЗржЯрж╛ржпрж╝ error-ржХрзЗ ржХржорж╛рзЯрзЗ рж░рж╛ржЦрзЗред 
\newline
SRM рж╕рзНржЯрзНрж░рж╛ржХржЪрж╛рж░рж╛рж▓ рж░рж┐рж╕рзНржХ ржорзВрж▓ржд ржПржорзНржкрж┐рж░рж┐ржХрзНржпрж╛рж▓ рж░рж┐рж╕рзНржХрзЗрж░ рж╕рж╛ржерзЗ ржПржХржЯрж┐ ржЕрждрж┐рж░рж┐ржХрзНржд ржкрзЗржирж╛рж▓рзНржЯрж┐ ржЯрж╛рж░рзНржо $\lambda J(f)$ ржпрзЛржЧ ржХрж░рзЗ ржпржЦржиржЗ ржоржбрзЗрж▓рзЗрж░ ржХржоржкрзНрж▓рзЗржХрзНрж╕рж┐ржЯрж┐ ржмрж╛рзЬрждрзЗ ржерж╛ржХрзЗред ржПржЦржи ржкрзНрж░рж╢рзНржи ржЖрж╕рзЗ, ржоржбрзЗрж▓рзЗрж░ ржХржоржкрзНрж▓рзЗржХрзНрж╕рж┐ржЯрж┐ ржмрж╛рзЬрж▓рзЗ ржХрж┐ рж╕ржорж╕рзНржпрж╛? ржПржХрзНрж╖рзЗрждрзНрж░рзЗ ржоржбрзЗрж▓ ржбрзЗржЯрж╛рж░ ржкрзНржпрж╛ржЯрж╛рж░рзНржирзЗрж░ ржкрж╛рж╢рж╛ржкрж╛рж╢рж┐ ржЕржкрзНрж░рзЯрзЛржЬржирзАрзЯ ржкрзНржпрж╛ржЯрж╛рж░рзНржиржУ ржзрж░рждрзЗ ржерж╛ржХржмрзЗ ржЬрзЗржЧрзБрж▓рзЛ ржорзВрж▓ржд ржирзЯрзЗрж╕ (noise)ред ржкрзЗржирж╛рж▓рзНржЯрж┐ ржЯрж╛рж░рзНржо ржПржХ ржзрж░ржирзЗрж░ ржмрзНржпрж╛рж▓рзЗржирзНрж╕ рждрзИрж░рж┐ ржХрж░рзЗ ржпрж╛рждрзЗ ржоржбрзЗрж▓ржЯрж┐ ржУржнрж╛рж░ржлрж┐ржЯ ржирж╛ ржХрж░рзЗред ERM ржПрж░ ржорждрзЛ ржоржбрзЗрж▓ ржЯрзНрж░рзЗржирж┐ржВ ржбрзЗржЯрж╛ржпрж╝ ржмрзЗрж╢рж┐ ржлрж┐ржЯ ржХрж░рж╛рж░ ржкрж╛рж╢рж╛ржкрж╛рж╢рж┐ ржоржбрзЗрж▓ржЯрж┐ ржпрзЗржи ржмрзЗрж╢рж┐ ржЬржЯрж┐рж▓ ржирж╛ рж╣ржпрж╝ рждрж╛ ржирж┐рж╢рзНржЪрж┐ржд SRM ред 
\begin{definition}


ERM(Empirical risk minimization)
\begin{equation} % 
\min\limits _{f \in \mathcal{F}} R_{\mathrm{emp}}(f)=\min\limits _{f \in \mathcal{F}} \dfrac{1}{N}\sum\limits_{i=1}^{N} L\left(y_i,f(x_i)\right)
\end{equation}

\begin{itemize}
    \item {\bengalifont N рж╕ржВржЦрзНржпржХ data ржерзЗржХрзЗ ржкрзНрж░рж╛ржкрзНржд рж▓рж╕ ржнрзНржпрж╛рж▓рзБ ржПрж░ ржЕрзНржпрж╛ржнрж╛рж░рзЗржЬ ржнрзНржпрж╛рж▓рзБ ржЧрзБрж▓рзЛрж░ ржоржзрзНржпрзЗ ржорж┐ржирж┐ржорж╛ржо ржпрзЗржЯрж╛ ржкрж╛ржм рж╣рж╛ржЗржкржерж┐рж╕рж┐рж╕ рж╕рзНржкрзЗржЗрж╕ ржерзЗржХрзЗ рж╕рзЗржЯрж╛ рж╣ржмрзЗ} $R_{\mathrm{emp}}$
\end{itemize}
\end{definition}

\begin{definition}
Structural risk
\begin{equation}
R_{\mathrm{smp}}(f)=\dfrac{1}{N}\sum\limits_{i=1}^{N} L\left(y_i,f(x_i)\right) +\lambda J(f)
\end{equation}
\end{definition}
\begin{itemize}
    \item $J(f)$ {\bengalifont ржПржоржи ржПржХржЯрж┐ ржЯрж╛рж░рзНржо ржпрж╛ ржмрзЗрж╢рж┐ ржЬржЯрж┐рж▓ ржоржбрзЗрж▓ржХрзЗ рж╢рж╛рж╕рзНрждрж┐ (penalty) ржжрзЗржпрж╝ }
    \item  $ \lambda $ {\bengalifont ржжрзНржмрж╛рж░рж╛ ржирж┐рж░рзНржзрж╛рж░рж┐ржд рж╣рзЯ ржХрждржЯрзБржХрзБ рж╢рж╛рж╕рзНрждрж┐ ржмрж╛ ржкрзЗржирж▓рж╛ржЗржЬ ржХрж░рж╛ рж╣ржмрзЗ ржХржоржкрзНрж▓рзЗржХрзНрж╕рж┐ржЯрж┐ рж▓рзЗржнрзЗрж▓ ржарж┐ржХ рж░рж╛ржЦрж╛рж░ ржЬржирзНржпрзЗ }
\end{itemize}

\begin{definition}
SRM(Structural risk minimization)
\bengalifont
SRM-ржПрж░ рж▓ржХрзНрж╖рзНржп рж╣рж▓ рж╕ржорж╕рзНржд рж╕ржорзНржнрж╛ржмрзНржп ржлрж╛ржВрж╢ржи $F$ ржерзЗржХрзЗ ржПржоржи ржПржХржЯрж┐ ржлрж╛ржВрж╢ржи $f$ ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж╛ ржпрж╛ ржПржорзНржкрж┐рж░рж┐ржХрж╛рж▓ рж░рж┐рж╕рзНржХ ржПржмржВ ржоржбрзЗрж▓ ржЬржЯрж┐рж▓рждрж╛рж░ рж╕ржорж╖рзНржЯрж┐ржХрзЗ рж╕рж░рзНржмржирж┐ржорзНржи ржХрж░рзЗред 

\begin{equation}
\min\limits _{f \in \mathcal{F}} R_{\mathrm{srm}}(f)=\min\limits _{f \in \mathcal{F}} \dfrac{1}{N}\sum\limits_{i=1}^{N} L\left(y_i,f(x_i)\right) +\lambda J(f)
\end{equation}


\begin{itemize}
    \item $R_{\mathrm{srm}}(f)$ {\bengalifont ржоржбрзЗрж▓рзЗрж░ рж╕рзНржЯрзНрж░рж╛ржХржЪрж╛рж░рж╛рж▓ рж░рж┐рж╕рзНржХ, ржпрж╛ ржПржорзНржкрж┐рж░рж┐ржХрж╛рж▓ рж░рж┐рж╕рзНржХ ржПржмржВ ржоржбрзЗрж▓рзЗрж░ ржЬржЯрж┐рж▓рждрж╛рж░ рж╕ржоржирзНржмржпрж╝рзЗ ржЧржарж┐рждред}
    \item $\dfrac{1}{N}\sum\limits_{i=1}^{N} L\left(y_i,f(x_i)\right)$ : {\bengalifont ржПржорзНржкрж┐рж░рж┐ржХрж╛рж▓ рж░рж┐рж╕рзНржХ, ржпрж╛ ржЯрзНрж░рзЗржирж┐ржВ ржбрзЗржЯрж╛ржпрж╝ ржоржбрзЗрж▓рзЗрж░ ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕рзЗрж░ ржЧржбрж╝ рждрзНрж░рзБржЯрж┐ ржорж╛ржкрж╛ рж╣ржпрж╝ред}
    \item $\lambda J(f)$ {\bengalifont рж░рзЗржЧрзБрж▓рж╛рж░рж╛ржЗржЬрзЗрж╢ржи ржЯрж╛рж░рзНржо, ржпрж╛ ржоржбрзЗрж▓рзЗрж░ ржЬржЯрж┐рж▓рждрж╛ржХрзЗ рж╢рж╛рж╕рзНрждрж┐ ржжрзЗржпрж╝ ржПржмржВ ржУржнрж╛рж░ржлрж┐ржЯрж┐ржВ ржПржбрж╝рж╛рждрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рзЗред}
\end{itemize}
\end{definition}


\subsection{Optimization}
\bengalifont
ржорзЗрж╢рж┐ржи рж▓рж╛рж░рзНржирж┐ржВ ржоржбрзЗрж▓ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯрзЗрж░ рж╕рж░рзНржмрж╢рзЗрж╖ ржзрж╛ржк рж╣ржЪрзНржЫрзЗ ржЕржкржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи (gradient descent), ржпрж╛рж░ ржоржзрзНржпржорзЗ рж╣рж╛ржЗржкржерж┐рж╕рж┐рж╕ рж╕рзНржкрзЗржЗрж╕ ржерзЗржХрзЗ рж╕рзЗрж░рж╛ ржХрзНрж▓рж╛рж╕рж┐ржлрж╛рзЯрж╛рж░ рж╕рж╛рж░рзНржЪ рж╕рзНржкрзЗржЗрж╕ ржерзЗржХрзЗ ржХржд ржХрж╛рж░рзНржпржХрж░рзАржнрж╛ржмрзЗ  



\section{{\bengalifont ржмрзНржпрж╛рж╕рж┐ржХ ржХржирж╕рзЗржкрзНржЯ}}


\subsection{Parametric vs non-parametric models}
\bengalifont
\textbf{{\bengalifont ржкрзНржпрж╛рж░рж╛ржорзЗржЯрзНрж░рж┐ржХ ржоржбрзЗрж▓:}} ржПржЧрзБрж▓рж┐рж░ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ рж╕ржВржЦрзНржпржХ ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ ржерж╛ржХрзЗред  ржоржбрзЗрж▓ржЯрж┐ ржПржХржмрж╛рж░ ржЯрзНрж░рзЗржЗржиржб рж╣ржпрж╝рзЗ ржЧрзЗрж▓рзЗ, ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ржЧрзБрж▓рж┐ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ рж╣ржпрж╝рзЗ ржпрж╛ржпрж╝ ржПржмржВ ржоржбрзЗрж▓рзЗрж░ ржХржоржкрзНрж▓рзЗржХрзНрж╕рж┐ржЯрж┐ ржмрж╛рзЬрзЗржирж╛ред ржпрзЗржоржи рж▓рж┐ржирж┐рзЯрж╛рж░ рж░рж┐ржЧрж░рзЗрж╢ржи, рж▓ржЬрж┐рж╕рзНржЯрж┐ржХ рж░рж┐ржЧрж░рзЗрж╢ржи

\textbf{{\bengalifont ржиржи-ржкрзНржпрж╛рж░рж╛ржорзЗржЯрзНрж░рж┐ржХ ржоржбрзЗрж▓:}} ржПржХрзНрж╖рзЗрждрзНрж░рзЗ ржоржбрзЗрж▓рзЗрж░ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ рж╕ржВржЦржХ ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ ржерж╛ржХрзЗ ржПржмржВ ржбрж╛ржЯрж╛рж╕рзЗржЯ ржмрзГржжрзНржзрж┐рж░ рж╕рж╛ржерзЗ рж╕рж╛ржерзЗ ржоржбрзЗрж▓рзЗрж░ ржХржорж▓рзЗржХрзНрж╕рж┐ржЯрж┐ ржмрж╛ ржЬржЯрж┐рж▓рждрж╛ ржмрж╛рзЬрждрзЗ ржерж╛ржХрзЗред  ржПржЧрзБрж▓рж┐ ржЖрж░ржУ ржлрзНрж▓рзЗржХрзНрж╕рж┐ржмрж▓ ржПржмржВ ржбрзЗржЯрж╛рж░ ржкрж░рж┐ржорж╛ржг ржмрзЗрж╢рж┐ ржерж╛ржХрж╛ рж▓рж╛ржЧрзЗред 

\subsection{{\bengalifont ржПржХржЯрж┐ рж╕рж╣ржЬ ржиржи-ржкрзНржпрж╛рж░рж╛ржорзЗржЯрзНрж░рж┐ржХ ржХрзНрж▓рж╛рж╕рж┐ржлрж╛ржпрж╝рж╛рж░:} k nearest algorithm }

\subsubsection{Representation}
\bengalifont
KNN ржПржХржЯрж┐ ржиржи-ржкрзНржпрж╛рж░рж╛ржорзЗржЯрзНрж░рж┐ржХ ржХрзНрж▓рж╛рж╕рж┐ржлрж╛ржпрж╝рж╛рж░ ржпрзЗржЦрж╛ржирзЗ ржПржХржЯрж┐ ржкржпрж╝рзЗржирзНржЯрзЗрж░ ржЖржЙржЯржкрзБржЯ рж╣ржпрж╝ рждрж╛рж░ рж╕ржмржЪрзЗржпрж╝рзЗ ржХрж╛ржЫрзЗрж░ $ЁЭСШ$ ржЯрж┐ ржкрзНрж░рждрж┐ржмрзЗрж╢рзАрж░ рж╕рж╛ржзрж╛рж░ржг рж╢рзНрж░рзЗржгрзАред
\begin{equation}
y=f(\vec{x})=\arg\min_{c}{\sum\limits_{\vec{x}_i \in N_k(\vec{x})} \mathbb{I}(y_i=c)}
\end{equation}
\bengalifont
ржпрзЗржЦрж╛ржирзЗ $N_k(\vec{x})$  $k$ ржкрзЯрзЗржирзНржЯрзЗрж░ ржПржХржЯрж┐ рж╕рзЗржЯ ржпрж╛рж░рж╛  $\vec{x}$ ржкрзЯрзЗржирзНржЯрзЗрж░ ржХрж╛ржЫрж╛ржХрж╛ржЫрж┐ред 
\begin{itemize}
    \item $N_k(\vec{x})$ {\bengalifont рж╣ржЪрзНржЫрзЗ ржкрзЯрзЗржирзНржЯ $X$ ржПрж░ ржЖрж╢рзЗржкрж╛рж╢рзЗрж░ $ЁЭСШ$ k-nearest neighbor}
    \item ${I}(y_i=c)$ {\bengalifont ржЗржирзНржбрж┐ржХрзЗржЯрж░ ржлрж╛ржВрж╢ржи ржпржжрж┐ $y_i$ c ржХрзНрж▓рж╛рж╕рзЗрж░ ржоржзрзНржпрзЗ ржкрзЬрзЗ рждржмрзЗ 1 рж░рж┐ржЯрж╛рж░рзНржи ржХрж░ржмрзЗ ржЖрж░ ржпржжрж┐ ржирж╛ рж╣ржпрж╝ рждржмрзЗ 0 рж░рж┐ржЯрж╛рж░рзНржи ржХрж░рзЗ} 
\end{itemize}

% Usually use \textbf{k-d tree} to accelerate the process of finding k nearest points.

ржЙржжрж╛рж╣рж░ржг: ржпржжрж┐ ЁЭСШ=3 рж╣ржпрж╝ ржПржмржВ x -ржПрж░ рж╕ржмржЪрзЗржпрж╝рзЗ ржХрж╛ржЫрзЗрж░ 3 ржЬржи ржкрзНрж░рждрж┐ржмрзЗрж╢рзАрж░ ржоржзрзНржпрзЗ ржжрзБржЯрж┐ рж╢рзНрж░рзЗржгрзА ЁЭР┤-рждрзЗ ржПржмржВ ржПржХржЯрж┐ рж╢рзНрж░рзЗржгрзА B-рждрзЗ ржерж╛ржХрзЗ, рждрж╛рж╣рж▓рзЗ ЁЭСж-ржПрж░ ржЖржЙржЯржкрзБржЯ рж╢рзНрж░рзЗржгрзА ЁЭР┤ рж╣ржмрзЗ, ржХрж╛рж░ржг ржПржЯрж┐ A-ржПрж░ ржкрзНрж░рждрж┐ржмрзЗрж╢рзАржжрзЗрж░ ржоржзрзНржпрзЗ рж╕ржмржЪрзЗржпрж╝рзЗ рж╕рж╛ржзрж╛рж░ржгред

% Example: If k=3 and among the 3 nearest neighbors of point x, two belong to class A and one belongs to class B, then the output class y will be A, because A is the most common class among the neighbors.

\subsubsection{Evaluation}
ржХрзЛржирзЛ ржЯрзНрж░рзЗржирж┐ржВ ржПрж░ ржкрзНрж░рзЯрзЛржЬржи рж╣рзЯ ржирж╛ред 

\subsubsection{Optimization}
ржХрзЛржирзЛ ржЯрзНрж░рзЗржирж┐ржВ ржПрж░ ржкрзНрж░рзЯрзЛржЬржи рж╣рзЯ ржирж╛ред 


\subsection{Overfitting}

ржУржнрж╛рж░ржлрж┐ржЯрж┐ржВ рж╣ржпрж╝ ржпржЦржи ржПржХржЯрж┐ ржоржбрзЗрж▓ ржЯрзНрж░рзЗржЗржиржб ржбрзЗржЯрж╛рждрзЗ ржЦрзБржм ржнрж╛рж▓рзЛ ржХрж╛ржЬ ржХрж░рзЗ ржХрж┐ржирзНрждрзБ ржирждрзБржи ржбрзЗржЯрж╛рждрзЗ ржЦрж╛рж░рж╛ржк ржХрж░рзЗред ржПржЯрж┐ ржЦрзБржм ржЬржЯрж┐рж▓ ржоржбрзЗрж▓ржЧрзБрж▓рж┐рждрзЗ ржШржЯрзЗ (complex model) ржпрж╛ ржбрзЗржЯрж╛рж░ noise-ржУ рж╢рж┐ржЦрзЗ ржлрзЗрж▓рзЗред



\subsection{Cross validation}
\label{sec:Cross-validation}
\begin{definition}
\textbf{Cross validation} {\bengalifont (ржЕржирзЗржХ рж╕ржоржпрж╝ ржпрж╛ржХрзЗ \emph{rotation estimation} ржмрж▓рж╛ рж╣ржпрж╝) рж╣рж▓ ржПржХржЯрж┐ \emph{model validation} ржкржжрзНржзрждрж┐, ржпрж╛ ржПржХржЯрж┐ statistical analysis ржлрж▓рж╛ржлрж▓ ржЕржирзНржп ржбрзЗржЯрж╛рж╕рзЗржЯрзЗ ржХрждржЯрж╛ ржнрж╛рж▓рзЛржнрж╛ржмрзЗ ржкрзНрж░ржпрж╝рзЛржЧ ржХрж░рж╛ ржпрж╛ржпрж╝ рждрж╛ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рждрзЗ ржмрзНржпржмрж╣рзГржд рж╣ржпрж╝ \footnote{\url{http://en.wikipedia.org/wiki/Cross-validation_(statistics)}}ред}
\end{definition}

{\bengalifont рж╕рж╛ржзрж╛рж░ржг ржХрж┐ржЫрзБ cross-validation ржПрж░ ржзрж░ржи:}
\begin{enumerate}
\item  K-fold cross-validation: {\bengalifont ржПржЗ ржкржжрзНржзрждрж┐рждрзЗ, ржорзВрж▓ sample-ржХрзЗ ржПрж▓рзЛржорзЗрж▓рзЛржнрж╛ржмрзЗ k рж╕ржорж╛ржи ржЖржХрж╛рж░рзЗрж░ subsample ржП ржнрж╛ржЧ ржХрж░рж╛ рж╣ржпрж╝ред ржПржЗ k ржЯрж┐ subsample ржПрж░ ржоржзрзНржпрзЗ ржПржХржЯрж┐ subsample ржоржбрзЗрж▓ржЯрж┐рждрзЗ test ржХрж░рж╛рж░ ржЬржирзНржп validation ржбрзЗржЯрж╛ рж╣рж┐рж╕рзЗржмрзЗ ржмрзНржпржмрж╣рзГржд рж╣ржпрж╝, ржЖрж░ ржмрж╛ржХрж┐ k тИТ 1 subsample ржЧрзБрж▓рзЛ ржоржбрзЗрж▓ржЯрж┐ training ржПрж░ ржЬржирзНржп ржмрзНржпржмрж╣рзГржд рж╣ржпрж╝ред}
\item  2-fold cross-validation: {\bengalifont simple k-fold cross-validation ржмрж▓рж╛ рж╣рзЯ , ржпрзЗржЦрж╛ржирзЗ k=2; holdout method ржУ ржмрж▓рж╛ рж╣ржпрж╝ред}
\item {\bengalifont Leave-one-out cross-validation(LOOCV)}: ржПржЦрж╛ржирзЗ k = M, ржЕрж░рзНржерж╛рзО ржорзВрж▓ sample ржПрж░ рж╕ржВржЦрзНржпрж╛ ржпрждред
\end{enumerate}

\subsection{Model selection}

{\bengalifont ржпржЦржи ржЖржорж╛ржжрзЗрж░ рж╣рж╛рждрзЗ ржмрж┐ржнрж┐ржирзНржи ржХржоржкрзНрж▓рзЗржХрзНрж╕ (ржпрзЗржоржи: ржнрж┐ржирзНржи ржнрж┐ржирзНржи degree ржПрж░ polynomials рж╕рж╣ linear ржмрж╛ logistic regression ржоржбрзЗрж▓, ржмрж╛ ржнрж┐ржирзНржи ржнрж┐ржирзНржи K ржПрж░ ржорж╛ржи рж╕рж╣ KNN classifiers) ржоржбрзЗрж▓рзЗрж░ ржЕржирзЗржХржЧрзБрж▓рзЛ ржмрж┐ржХрж▓рзНржк ржерж╛ржХрзЗ, рждржЦржи ржЖржорж░рж╛ рж╕ржарж┐ржХ ржоржбрзЗрж▓ржЯрж┐ ржХрзАржнрж╛ржмрзЗ ржирж┐рж░рзНржзрж╛рж░ржи ржХрж░ржм? ржПржХржЯрж┐ рж╕рж╛ржзрж╛рж░ржг ржЙржкрж╛рзЯ рж╣рж▓, ржкрзНрж░рждрж┐ржЯрж┐ ржкржжрзНржзрждрж┐рж░ ржЬржирзНржп training set ржП \textbf{misclassification rate} рж╣рж┐рж╕рж╛ржм ржХрж░рж╛ред}


% \subsection{Cross validation}
% \label{sec:Cross-validation}
% \begin{definition}
% \textbf{Cross validation}, sometimes called \emph{rotation estimation}, is a \emph{model validation} technique for assessing how the results of a statistical analysis will generalize to an independent data set\footnote{\url{http://en.wikipedia.org/wiki/Cross-validation_(statistics)}}.
% \end{definition}

% Common types of cross-validation:
% \begin{enumerate}
% \item K-fold cross-validation. In k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k тИТ 1 subsamples are used as training data.
% \item 2-fold cross-validation. Also, called simple cross-validation or holdout method. This is the simplest variation of k-fold cross-validation, k=2.
% \item Leave-one-out cross-validation(\emph{LOOCV}). k=M, the number of original samples.
% \end{enumerate}


% \subsection{Model selection}

% When we have a variety of models of different complexity (e.g., linear or logistic regression models with different degree polynomials, or KNN classifiers with different values ofK), how should we pick the right one? A natural approach is to compute the \textbf{misclassification rate} on the training set for each method.

\end{document}




















